{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RFnPSdxLC6em"
   },
   "source": [
    "# Analysing H1B Acceptance Trends "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uh8CJdwBJOAL"
   },
   "source": [
    "H1B visa is a nonimmigrant visa issued to gradute level workers which allows them to work in the United States. The employer sponsors the H1B visa for workers with theoretical or technical expertise in specialized fields such as in IT, finance, accounting etc. An interesting fact about immigrant workers is that about 52 percent of new Silicon valley companies were founded by such workers during 1995 and 2005. Some famous CEOs like Indira Nooyi (Pepsico), Elon Musk (Tesla), Sundar Pichai (Google),Satya Nadella (Microsoft) once arrived to the US on a H1B visa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QIe1lq1-RyDV"
   },
   "source": [
    "**Motivation**: Our team consists of five international gradute students, in the future we will be applying for H1B visa. The visa application process seems very long, complicated and uncertain. So we decided to understand this process and use Machine learning algorithms to predict the acceptance rate and trends of H1B visa. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kjQ8YI2GTfb6"
   },
   "source": [
    "## Data \n",
    "The data used in the project has been collected from <a href=\"https://www.foreignlaborcert.doleta.gov/performancedata.cfm\">the Office of Foreign Labor Certification (OFLC).</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "eUglaNmlQ44L",
    "outputId": "b915b8d8-490d-4bf4-98db-8e8151e59922"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "_sSzHVSl3-bD",
    "outputId": "ab9bf144-a50d-4be5-85fe-0c16d1d97e74"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install autocorrect\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from autocorrect import Speller \n",
    "nltk.download('wordnet')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FARuLUkuV7T8"
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pleklaelV_xO"
   },
   "source": [
    "Before we begin working on our data we need to understand the traits of our data which we accomplish using EDA. We see that we have about 260 columns , not all 260 columns have essential information that contributes to our analysis. Hence we pick out the columns such as case status( Accepted/ Denied) ,Employer, Job title etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "iCiYlOyXHHKJ",
    "outputId": "39df68e3-d8df-4797-af48-aa2d5bf6f29e"
   },
   "outputs": [],
   "source": [
    "#Read the csv file and stored in file\n",
    "file=pd.read_csv('/content/gdrive/My Drive/H-1B_Disclosure_Data_FY2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XT6Wmc2-WVo7",
    "outputId": "b1f659c9-cd9b-4f16-961c-6f78e6eb95a7"
   },
   "outputs": [],
   "source": [
    "file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "id": "ZxxeAh5SX0Bc",
    "outputId": "1346c035-881c-42fc-83b5-e88749c9ca61"
   },
   "outputs": [],
   "source": [
    "cleaned=file[['CASE_NUMBER','CASE_STATUS','CASE_SUBMITTED','DECISION_DATE','VISA_CLASS','FULL_TIME_POSITION','JOB_TITLE','SOC_CODE','SOC_TITLE',\\\n",
    "              'EMPLOYER_NAME','WAGE_RATE_OF_PAY_FROM_1','WAGE_UNIT_OF_PAY_1','NAICS_CODE','WORKSITE_CITY_1','WORKSITE_STATE_1']]\n",
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7sKPhmuNf6eW",
    "outputId": "ff0ef8b5-24fd-4650-81d6-831c5cbfe2f9"
   },
   "outputs": [],
   "source": [
    "cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "BbgGJEB9XpO6",
    "outputId": "77d86b2c-a433-4b9a-e0b4-6001a3185f35"
   },
   "outputs": [],
   "source": [
    "cleaned['VISA_CLASS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "NLttcaMGZC1l",
    "outputId": "93927a2a-1a78-47f7-a6b9-b9e69d755472"
   },
   "outputs": [],
   "source": [
    "# Visa class has many categories which are not of use , we require only H1B visa type , hence we drop all records with other visa types\n",
    "cleaned.drop(labels=cleaned.loc[cleaned['VISA_CLASS']!='H-1B'].index , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "3dzct3CLeVuI",
    "outputId": "58d3a1a4-9fce-4aa9-adf5-aac08a11ce31"
   },
   "outputs": [],
   "source": [
    "cleaned['FULL_TIME_POSITION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "qox8ruM-Xff_",
    "outputId": "fbe21f04-bcff-4eea-e397-81adb5cde617"
   },
   "outputs": [],
   "source": [
    "cleaned['CASE_STATUS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "colab_type": "code",
    "id": "7Cz9Kkghr3JP",
    "outputId": "e682961a-67ef-4fa4-fdab-33590860ee78"
   },
   "outputs": [],
   "source": [
    "#As we want to only need accepted and denied cases we are dropping withdrawn from the data frame. \n",
    "#Case status of class certified-withdraw were certified earlier and later withdraw which can be considered a\n",
    "cleaned.replace({\"CASE_STATUS\":\"CERTIFIED-WITHDRAWN\"},\"CERTIFIED\",inplace=True)\n",
    "cleaned.drop(labels=cleaned.loc[cleaned['CASE_STATUS']=='WITHDRAWN'].index , inplace=True)\n",
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "75W7CN49Yvhf"
   },
   "outputs": [],
   "source": [
    "#cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "F5SSKy8mVr01",
    "outputId": "3e9ae2fd-d038-4dab-80ea-398bdcb2c5b6"
   },
   "outputs": [],
   "source": [
    "#the column wages has a mix of both string and float value types and some record have the symbol '$' which we want to remove\n",
    "cleaned['WAGE_RATE_OF_PAY_FROM_1'].apply(type).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "dxghSqYygMTm",
    "outputId": "bead1f17-f9e6-453c-990e-2147b7b5b499"
   },
   "outputs": [],
   "source": [
    "cleaned['WORKSITE_STATE_1'].apply(type).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cDAglVs6XHVB"
   },
   "outputs": [],
   "source": [
    "def clean_wages(w):\n",
    "    \"\"\" Function to remove '$' symbol and other delimiters from wages column which consistes of str and float type values\n",
    "    if the column entry is string type then remove the symbols else return the column value as it is \n",
    "    \"\"\"\n",
    "    if isinstance(w, str):\n",
    "        return(w.replace('$', '').replace(',', ''))\n",
    "    return(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "Py3xCbSawIpR",
    "outputId": "f9f0c191-256b-4370-a930-33a5f045923f"
   },
   "outputs": [],
   "source": [
    "cleaned['WAGES']=cleaned['WAGE_RATE_OF_PAY_FROM_1'].apply(clean_wages).astype('float')\n",
    "#cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "yKCdooZsQ7yD",
    "outputId": "ad04e651-10bf-471a-cf3c-5934ca0e6a5e"
   },
   "outputs": [],
   "source": [
    "# the wage information that we have available has different unit of pay\n",
    "cleaned['WAGE_UNIT_OF_PAY_1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "hCGTUT22Z6Bj",
    "outputId": "6b792ce6-c6ac-4b53-9fed-43336442ee2b"
   },
   "outputs": [],
   "source": [
    "# we convert the different units of pay to the type 'Year'\n",
    "cleaned['WAGES'] = np.where(cleaned['WAGE_UNIT_OF_PAY_1'] == 'Month',cleaned['WAGES'] * 12,cleaned['WAGES'])\n",
    "cleaned['WAGES'] = np.where(cleaned['WAGE_UNIT_OF_PAY_1'] == 'Hour',cleaned['WAGES'] * 2080,cleaned['WAGES']) # 2080=8 hours*5 days* 52 weeks\n",
    "cleaned['WAGES'] = np.where(cleaned['WAGE_UNIT_OF_PAY_1'] == 'Bi-Weekly',cleaned['WAGES'] *26,cleaned['WAGES'])\n",
    "cleaned['WAGES'] = np.where(cleaned['WAGE_UNIT_OF_PAY_1'] == 'Week',cleaned['WAGES'] * 52,cleaned['WAGES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "1Q006QHRdl-q",
    "outputId": "17b7585f-6380-4895-b96f-773d5f5f755f"
   },
   "outputs": [],
   "source": [
    "#As we have got the information of Wages and made transformation we can drop the initial two records\n",
    "cleaned.drop(columns=['WAGE_RATE_OF_PAY_FROM_1','WAGE_UNIT_OF_PAY_1'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "lnSktMB3bVM3",
    "outputId": "6236df39-d468-4e80-a870-20d8dda250f0"
   },
   "outputs": [],
   "source": [
    "cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "RyS7f3eIbcCl",
    "outputId": "6f3fe01f-f3c8-4ffe-d700-a3c1c37fb32b"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We should remove record that have null objects, from the above cell we see\n",
    "that all columns don't have same number of non-null records\n",
    "which means we have to remove the records that have the null values.\n",
    "we see that there are about 17 records that have null values\n",
    "\"\"\" \n",
    "null_rows = cleaned.isnull().any(axis=1)\n",
    "print(cleaned[null_rows].shape)\n",
    "print(cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "keAByWsddXRe",
    "outputId": "0acbb87c-2941-4dab-ca72-52d630d9ef3a"
   },
   "outputs": [],
   "source": [
    "cleaned.dropna(inplace=True)\n",
    "print(cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Aa-4cFWnW6s"
   },
   "outputs": [],
   "source": [
    "#cleaned['JOB_TITLE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "id": "M03_5gLDxgYT",
    "outputId": "d33dada3-a5f0-467f-cbf8-da7e773fef85"
   },
   "outputs": [],
   "source": [
    "#we see that the job title has integers(words with integers also) \n",
    "#removing comma also\n",
    "def remove_num(text):\n",
    "  if not any(c.isdigit() for c in text):\n",
    "    return text\n",
    "  return ''\n",
    "cleaned['JOB_TITLE']=cleaned.JOB_TITLE.apply(lambda txt: \" \".join([remove_num(i) for i in txt.lower().split()]))\n",
    "cleaned['JOB_TITLE']=cleaned['JOB_TITLE'].str.replace(',', '')\n",
    "cleaned['SOC_TITLE']=cleaned.SOC_TITLE.apply(lambda txt: \" \".join([remove_num(i) for i in txt.lower().split()]))\n",
    "cleaned['SOC_TITLE']=cleaned['SOC_TITLE'].str.replace(',', '')\n",
    "print(\"Numbers and strings with numbers removed\" )\n",
    "#cleaned.head()\n",
    "#cleaned['JOB_TITLE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "3DxeRz6Mxkx1",
    "outputId": "841fb983-3182-48d0-9c65-cb5f134b31ac"
   },
   "outputs": [],
   "source": [
    "nltk.download('words')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "words = set(nltk.corpus.words.words())\n",
    "spell = Speller()\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "  return lemmatizer.lemmatize(text)\n",
    "\n",
    "def spelling_checker(text):\n",
    "  return spell(text)\n",
    " \n",
    "print(spelling_checker(\"computr sciece progam check\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "5ZzB9FbCxvgQ",
    "outputId": "60e744c9-8c6d-4300-fd08-d0a8589fa2c4"
   },
   "outputs": [],
   "source": [
    "#this part takes more time because spell_checker \n",
    "cleaned['JOB_TITLE']=cleaned.JOB_TITLE.apply(lambda txt: \" \".join([lemmatize_text(i) for i in txt.lower().split()]))\n",
    "print(' JOB_TITLE IS  lemmatized')\n",
    "#print(cleaned['JOB_TITLE'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned['JOB_TITLE']=cleaned.JOB_TITLE.apply(lambda txt: \" \".join([spelling_checker(i) for i in txt.lower().split()]))\n",
    "print('JOB_TITLE SPELLING MISTAKES RECTIFIED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "q1NGrEDGxzPS",
    "outputId": "a0d529f0-23e8-425a-b01a-8359cdd0adbe"
   },
   "outputs": [],
   "source": [
    "#clean SOC TITLE\n",
    "cleaned['SOC_TITLE']=cleaned.SOC_TITLE.apply(lambda txt: \" \".join([lemmatize_text(i) for i in txt.lower().split()]))\n",
    "cleaned['SOC_TITLE']=cleaned.SOC_TITLE.apply(lambda txt: \" \".join([spelling_checker(i) for i in txt.lower().split()]))\n",
    "print('SOC_TITLE SPELLING MISTAKES RECTIFIED')\n",
    "cleaned = cleaned.groupby(\"SOC_TITLE\").filter(lambda x: len(x) > 15)\n",
    "print(\" removing least significant values from SOC_TITLE\")\n",
    "#cleaned['SOC_TITLE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = cleaned.groupby(\"SOC_CODE\").filter(lambda x: len(x) > 15)\n",
    "print(\"DROPPING THE LEAST SIGNIFICANT EMPLYOERS\")\n",
    "#cleaned['SOC_CODE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bwgV1bu5sXlE"
   },
   "outputs": [],
   "source": [
    "#we see that the job title has integers in the record which we can remove\n",
    "#handeled above\n",
    "#so commenting this part\n",
    "#cleaned['JOB_TITLE']=cleaned['JOB_TITLE'].str.replace('[0-9(){}[].]', '')\n",
    "#cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "id": "15py5C4RfXIj",
    "outputId": "f626e7a9-746a-4dbe-e640-ac630468f976"
   },
   "outputs": [],
   "source": [
    "cleaned['SOC_TITLE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned['EMPLOYER_NAME'].value_counts()\n",
    "cleaned = cleaned.groupby(\"EMPLOYER_NAME\").filter(lambda x: len(x) > 15)\n",
    "print(\"DROPPING THE LEAST SIGNIFICANT EMPLYOERS\")\n",
    "#cleaned['EMPLOYER_NAME'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "colab_type": "code",
    "id": "67Q2TX1Ju18U",
    "outputId": "b5331455-bda4-4a0a-a50f-9f829f57ab19"
   },
   "outputs": [],
   "source": [
    "Top_Employer=cleaned['EMPLOYER_NAME'].value_counts()[:10]\n",
    "Top_Employer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "1z8j5d55v2Ak",
    "outputId": "75bf36b7-4236-4c65-c496-ee6008e21b4a"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "colab_type": "code",
    "id": "IARZcE10wHBl",
    "outputId": "fad8582b-35a0-40b2-f1ea-b906f8fe7f6d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,10])\n",
    "ax=sns.barplot(y=Top_Employer.index,x=Top_Employer.values,palette=sns.color_palette('viridis',10))\n",
    "ax.tick_params(labelsize=12)\n",
    "for i, v in enumerate(Top_Employer.values): \n",
    "    ax.text(.5, i, v,fontsize=15,color='white',weight='bold')\n",
    "plt.title('Top 10 Companies sponsoring H1B Visa in 2019', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SBHPirCRabjw"
   },
   "outputs": [],
   "source": [
    "def wage_feature_eng(wage):\n",
    "    if wage <=50000:\n",
    "        return \"VERY LOW\"\n",
    "    elif wage in range(50000,75000):\n",
    "        return \"LOW\"\n",
    "    elif wage in range(75000,100000):\n",
    "        return \"AVERAGE\"\n",
    "    elif wage in range(100000,150000):\n",
    "        return \"HIGH\"\n",
    "    elif wage >=150000:\n",
    "        return \"VERY HIGH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 552
    },
    "colab_type": "code",
    "id": "EYUdOnMdafXp",
    "outputId": "c3c677d7-68ba-40b8-b4a4-53764aae950e"
   },
   "outputs": [],
   "source": [
    "cleaned['WAGE_CATEGORY'] = cleaned['WAGES'].apply(wage_feature_eng)\n",
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BEFORE CLEANING THE WORKSITE_STATE_1 COLUMN\")\n",
    "cleaned[\"WORKSITE_STATE_1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to clean the \"WORKSITE_STATE_1\" column because some of the values are abbrevations and some of them are names\n",
    "#So changed to names as most of the values are names.   \n",
    "print(\"AFTER CLEANING THE WORKSITE_STATE_1 COLUMN\")\n",
    "\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"AL\"),\"WORKSITE_STATE_1\"] = \"ALABAMA\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"AK\"),\"WORKSITE_STATE_1\"] = \"ALASKA\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"AZ\"),\"WORKSITE_STATE_1\"] = \"ARIZONA\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"AR\"),\"WORKSITE_STATE_1\"] = \"ARKANSAS\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"CA\"),\"WORKSITE_STATE_1\"] = \"CALIFORNIA\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"CO\"),\"WORKSITE_STATE_1\"] = \"COLORADO\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"DE\"),\"WORKSITE_STATE_1\"] = \"DELAWARE\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"FL\"),\"WORKSITE_STATE_1\"] = \"FLORIDA\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"GA\"),\"WORKSITE_STATE_1\"] = \"GEORGIA\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"HI\"),\"WORKSITE_STATE_1\"] = \"HAWAII\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"ID\"),\"WORKSITE_STATE_1\"] = \"IDAHO\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"IL\"),\"WORKSITE_STATE_1\"] = \"ILLINOIS\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"IN\"),\"WORKSITE_STATE_1\"] = \"INDIANA\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"IA\"),\"WORKSITE_STATE_1\"] = \"IOWA\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"KS\"),\"WORKSITE_STATE_1\"] = \"KANSAS\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"KY\"),\"WORKSITE_STATE_1\"] = \"KENTUCKY\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"LA\"),\"WORKSITE_STATE_1\"] = \"LOUISIANA\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"ME\"),\"WORKSITE_STATE_1\"] = \"MAINE\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"MD\"),\"WORKSITE_STATE_1\"] = \"MARYLAND\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"MA\"),\"WORKSITE_STATE_1\"] = \"MASSACHUSETTS\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"MI\"),\"WORKSITE_STATE_1\"] = \"MICHIGAN\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"MN\"),\"WORKSITE_STATE_1\"] = \"MINNESOTA\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"MS\"),\"WORKSITE_STATE_1\"] = \"MISSISSIPPI\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"MO\"),\"WORKSITE_STATE_1\"] = \"MISSOURI\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"MT\"),\"WORKSITE_STATE_1\"] = \"MONTANA\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"NE\"),\"WORKSITE_STATE_1\"] = \"NEBRASKA\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"NV\"),\"WORKSITE_STATE_1\"] = \"NEVADA\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"NH\"),\"WORKSITE_STATE_1\"] = \"NEW HAMPSHIRE\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"NJ\"),\"WORKSITE_STATE_1\"] = \"NEW JERSEY\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"NM\"),\"WORKSITE_STATE_1\"] = \"NEW MEXICO\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"NY\"),\"WORKSITE_STATE_1\"] = \"NEW YORK\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"NC\"),\"WORKSITE_STATE_1\"] = \"NORTH CAROLINA\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"ND\"),\"WORKSITE_STATE_1\"] = \"NORTH DAKOTA\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"OH\"),\"WORKSITE_STATE_1\"] = \"OHIO\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"OK\"),\"WORKSITE_STATE_1\"] = \"OKLAHOMA\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"OR\"),\"WORKSITE_STATE_1\"] = \"OREGON\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"PA\"),\"WORKSITE_STATE_1\"] = \"PENNSYLVANIA\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"RI\"),\"WORKSITE_STATE_1\"] = \"RHODE ISLAND\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"SC\"),\"WORKSITE_STATE_1\"] = \"SOUTH CAROLINA\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"SD\"),\"WORKSITE_STATE_1\"] = \"SOUTH DAKOTA\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"TN\"),\"WORKSITE_STATE_1\"] = \"TENNESSEE\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"TX\"),\"WORKSITE_STATE_1\"] = \"TEXAS\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"UT\"),\"WORKSITE_STATE_1\"] = \"UTAH\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"VT\"),\"WORKSITE_STATE_1\"] = \"VERMONT\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"VA\"),\"WORKSITE_STATE_1\"] = \"VIRGINIA\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"WA\"),\"WORKSITE_STATE_1\"] = \"WASHINGTON\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"WV\"),\"WORKSITE_STATE_1\"] = \"WEST VIRGINIA\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"WI\"),\"WORKSITE_STATE_1\"] = \"WISCONSIN\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"WY\"),\"WORKSITE_STATE_1\"] = \"WYOMING\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"PR\"),\"WORKSITE_STATE_1\"] = \"PUERTO RICO\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"VI\"),\"WORKSITE_STATE_1\"] = \"U.S. VIRGIN ISLANDS\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"MP\"),\"WORKSITE_STATE_1\"] = \"NORTHERN MARIANA ISLANDS\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"GU\"),\"WORKSITE_STATE_1\"] = \"GUAM\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"MH\"),\"WORKSITE_STATE_1\"] = \"MARSHALL ISLANDS\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"PW\"),\"WORKSITE_STATE_1\"] = \"PALAU\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"DC\"),\"WORKSITE_STATE_1\"] = \"DISTRICT OF COLUMBIA\"\n",
    "cleaned.loc[(cleaned.WORKSITE_STATE_1 == \"CT\"),\"WORKSITE_STATE_1\"] = \"CONNECTICUT\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned[\"WORKSITE_STATE_1\"].value_counts()\n",
    "print(\"CONVERTING CATEGORICAL COLUMNS INTO NUMERIC COLUMNS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = cleaned.groupby(\"WORKSITE_STATE_1\").filter(lambda x: len(x) > 15)\n",
    "print(\"DROPPING LEAST SIGNIFICANT STATES\")\n",
    "#cleaned[\"WORKSITE_STATE_1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERTING CATEGORICAL COLUMNS INTO NUMERIC COLUMNS\n",
    "print(cleaned[\"CASE_STATUS\"].value_counts())\n",
    "cleaned.loc[(cleaned.CASE_STATUS == \"CERTIFIED\"),\"CASE_STATUS\"] = 1\n",
    "cleaned.loc[(cleaned.CASE_STATUS == \"DENIED\"),\"CASE_STATUS\"] = 0\n",
    "print(cleaned[\"CASE_STATUS\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleaned[\"FULL_TIME_POSITION\"].value_counts())\n",
    "cleaned.loc[(cleaned.FULL_TIME_POSITION == \"Y\"),\"FULL_TIME_POSITION\"] = 1\n",
    "cleaned.loc[(cleaned.FULL_TIME_POSITION == \"N\"),\"FULL_TIME_POSITION\"] = 0\n",
    "print(cleaned[\"FULL_TIME_POSITION\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline classifier\n",
    "\n",
    "The baseline classifier is done with a basic model. In this case we are taking the mean of the labels ('certified' and 'denied' for H1B visa approvals). It will give us the base accuracy to which we will compare our classifier's accuracy. Our classifier should have a better accuracy than the baseline classifier accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step assigns a binary class label (0 or 1) to each label for H1B visa approval. \n",
    "#'CERTIFIED' is mapped to 1 and 'DENIED' to 0\n",
    "\n",
    "def create_class_labels(processed_data):\n",
    "    \n",
    "    y = np.where((processed_data['CASE_STATUS']=='CERTIFIED'),1, 0)\n",
    "    \n",
    "    return y\n",
    "\n",
    "X = cleaned['CASE_STATUS'].to_numpy()\n",
    "\n",
    "# Groundtruth labels for the dataset\n",
    "y = create_class_labels(cleaned)\n",
    "counts = cleaned['CASE_STATUS'].value_counts()\n",
    "print(counts)\n",
    "print('proportion: ', counts[0]/counts[1], ': 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from statistics import mean\n",
    "\n",
    "# Baseline classifier that predicts the class base on the mode of the labels.\n",
    "\n",
    "class BaselineClasifier():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.central_tendency = None\n",
    "        \n",
    "    def fit(self, data, y, central_t='mode'): \n",
    "        \n",
    "        # Count labels and find the most frequent one\n",
    "        label, counts = np.unique(y, return_counts=True) \n",
    "        \n",
    "        if central_t == 'mode':\n",
    "            self.central_tendency = counts.argmax()\n",
    "        elif central_t == 'mean':\n",
    "            self.central_tendency = round(np.sum(y)/len(y))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    # Return an array with size equal to the data size  and each element setted to the mode.\n",
    "    def predict(self, data):\n",
    "        \n",
    "        result = np.full(data.shape[0], self.central_tendency)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(validation, predicted):\n",
    "    \n",
    "    comp = prediction == validation \n",
    "    match_counts = np.count_nonzero(comp == True) \n",
    "    clasifier_accuracy = match_counts/len(validation)\n",
    "    \n",
    "    return clasifier_accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def compute_AUC(y, prediction):\n",
    "    \n",
    "    auc = roc_auc_score(y, prediction)\n",
    "\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "# Testing with K-folds\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "kf = sklearn.model_selection.KFold(n_splits=4, random_state=1, shuffle=True) \n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx] \n",
    "    baseline_clasifier = BaselineClasifier()\n",
    "    classifier = baseline_clasifier.fit(X_train, y_train, 'mean')\n",
    "    prediction = baseline_clasifier.predict(X_test)\n",
    "    \n",
    "    fold_accuracy = compute_accuracy(y_test, prediction)\n",
    "    fold_AUC = compute_AUC(y_test, prediction)\n",
    "    accuracies.append(fold_accuracy)\n",
    "    \n",
    "baseline_clasifier_accuracy = mean(accuracies)\n",
    "\n",
    "print('Baseline accuracy: ', baseline_clasifier_accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Testing with regular split\n",
    "\n",
    "# create training and testing vars\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "baseline_clasifier = BaselineClasifier()\n",
    "classifier = baseline_clasifier.fit(X_train, y_train, 'mean')\n",
    "prediction = baseline_clasifier.predict(X_test)\n",
    "\n",
    "split_accuracy = compute_accuracy(y_test, prediction)\n",
    "split_AUC = compute_AUC(y_test, prediction)\n",
    "\n",
    "print('Baseline accuracy: ', split_accuracy)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy results of the baseline classifier is 0.99. This result is due to the highly imbalanced data, where there are 624682 CERTIFIED applications and 5158 DENIED applications. The proportion of it is 121.10934470725087 to 1. Therefore, a performance measure based on the accuracy is not a good one. A better performance measure in imbalanced data is the Area under the ROC Curve (AUC). It meassures the likelihood that given two random points (one from the positive and one from the negative class) the classifier will rank the point from the positive class higher than the one from the negative one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('K-fold: ', fold_AUC)\n",
    "print('split (80-20): ', split_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONE HOT ENCODING ON SOC_CODE column\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "Dataset = cleaned[[\"CASE_STATUS\", \"FULL_TIME_POSITION\", \"SOC_CODE\", \"SOC_TITLE\", \"EMPLOYER_NAME\", \"WORKSITE_STATE_1\", \"WAGES\"]]\n",
    "Dataset.head()\n",
    "SOC_Encoding = OneHotEncoder(handle_unknown='ignore',sparse = True)\n",
    "SOC_Encoding_df = pd.DataFrame(SOC_Encoding.fit_transform(cleaned[[\"SOC_CODE\"]]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SOC_Encoding_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependent and independent variables\n",
    "Y = Dataset['CASE_STATUS'].values\n",
    "X = SOC_Encoding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the date into training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Build model - Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#model_clf = RandomForestClassifier(n_estimators=10, random_state=30)\n",
    "model_clf = RandomForestClassifier(n_jobs=2,random_state=0)\n",
    "\n",
    "#train the model\n",
    "model_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test the model (predict with our test data)\n",
    "prediction_test = model_clf.predict(X_test)\n",
    "prediction_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#compare with original value, Y_test\n",
    "from sklearn import metrics\n",
    "print(\"Accuracy = \", metrics.accuracy_score(y_test, prediction_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Test a simple input to predict its STATUS.\n",
    "status_label = np.array(['Denied','Approved'])\n",
    "new_pred_number = model_clf.predict([\n",
    "    [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "    0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "    0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "    0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "    0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "    0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "    0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]])\n",
    "new_pred_label= status_label[ new_pred_number ]\n",
    "new_pred_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "\n",
    "What is the hardest part of the project that you’ve encountered so far?\n",
    "\n",
    "1. Setting up the data for visualization and ML analysis, e.g. same job title is cluttered with different words, integers, and punctuation characters. \n",
    "2. Encoding the dataset to be used in the Classifier. We tried with JOB_TITLE attribute but got Memory error, instead, we started to use SOC_TITLE attribute"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "H1b_project.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
