{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RFnPSdxLC6em"
   },
   "source": [
    "## Analysing H1B Acceptance Trends "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uh8CJdwBJOAL"
   },
   "source": [
    "H1B visa is a nonimmigrant visa issued to gradute level applicants allowing them to work in the United States. The employer sponsors the H1B visa for workers with theoretical or technical expertise in specialized fields such as in IT, finance, accounting etc. An interesting fact about immigrant workers is that about 52 percent of new Silicon valley companies were founded by such workers during 1995 and 2005. Some famous CEOs like Indira Nooyi (Pepsico), Elon Musk (Tesla), Sundar Pichai (Google),Satya Nadella (Microsoft) once arrived to the US on a H1B visa.\\\n",
    "**Motivation**: Our team consists of five international gradute students, in the future we will be applying for H1B visa. The visa application process seems very long, complicated and uncertain. So we decided to understand this process and use Machine learning algorithms to predict the acceptance rate and trends of H1B visa. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kjQ8YI2GTfb6"
   },
   "source": [
    "### Data \n",
    "The data used in the project has been collected from <a href=\"https://www.foreignlaborcert.doleta.gov/performancedata.cfm\">the Office of Foreign Labor Certification (OFLC).</a>The Data provides insight into each petition with information such as the Job title, Wage, Employer, Worksite location etc. To get the dataset click on the above link-> click on Disclosure data -> scroll down to H1B data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZNTPm7WVCsm4"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_sSzHVSl3-bD"
   },
   "outputs": [],
   "source": [
    "!pip install autocorrect\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk,warnings\n",
    "import clean_wage as cw\n",
    "%matplotlib inline\n",
    "import remove\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from statistics import mean\n",
    "nltk.download('words')\n",
    "from autocorrect import Speller \n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import OneHotEncoder #ONE HOT ENCODING\n",
    "from sklearn.ensemble import RandomForestClassifier #Build model - Random Forest Classifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FARuLUkuV7T8"
   },
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pleklaelV_xO"
   },
   "source": [
    "Before we begin working on our data we need to understand the traits of our data which we accomplish using EDA. We see that we have about 260 columns , not all 260 columns have essential information that contributes to our analysis. Hence we pick out the columns such as case status( Accepted/ Denied) ,Employer, Job title etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iCiYlOyXHHKJ"
   },
   "outputs": [],
   "source": [
    "file=pd.read_csv('/content/gdrive/My Drive/H-1B_Disclosure_Data_FY2019.csv')#Read the csv file and store it in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "ZxxeAh5SX0Bc",
    "outputId": "22c3413b-f295-4266-c93b-793468bb23e4"
   },
   "outputs": [],
   "source": [
    "cleaned=file[['CASE_NUMBER','CASE_STATUS','CASE_SUBMITTED','DECISION_DATE','VISA_CLASS','FULL_TIME_POSITION','JOB_TITLE','SOC_CODE','SOC_TITLE',\\\n",
    "              'EMPLOYER_NAME','WAGE_RATE_OF_PAY_FROM_1','WAGE_UNIT_OF_PAY_1','NAICS_CODE','WORKSITE_CITY_1','WORKSITE_STATE_1']]\n",
    "cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98
    },
    "colab_type": "code",
    "id": "BbgGJEB9XpO6",
    "outputId": "cf634aa3-4ba3-43fc-fc46-6b53feecb9d9"
   },
   "outputs": [],
   "source": [
    "print(cleaned['VISA_CLASS'].value_counts()) # similarly we can find the categories in CASE_STATUS and 'FULL_TIME_POSITION'\n",
    "# Visa class has many categories which are not of use , we require only H1B visa type , hence we drop all records with other visa types\n",
    "cleaned.drop(labels=cleaned.loc[cleaned['VISA_CLASS']!='H-1B'].index , inplace=True)\n",
    "#In case status we can drop withdrawn records and we can change certified-withdrawn to certified\n",
    "cleaned.replace({\"CASE_STATUS\":\"CERTIFIED-WITHDRAWN\"},\"CERTIFIED\",inplace=True)\n",
    "cleaned.drop(labels=cleaned.loc[cleaned['CASE_STATUS']=='WITHDRAWN'].index , inplace=True)\n",
    "cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66
    },
    "colab_type": "code",
    "id": "F5SSKy8mVr01",
    "outputId": "02b29bf5-356d-46ce-ec35-c04e38d96995"
   },
   "outputs": [],
   "source": [
    "#the column wages has a mix of both string and float value types and some record have the symbol '$' which we want to remove\n",
    "cleaned['WAGE_RATE_OF_PAY_FROM_1'].apply(type).value_counts()\n",
    "cleaned['WAGES']=cleaned['WAGE_RATE_OF_PAY_FROM_1'].apply(cw.clean_wages).astype('float')\n",
    "cleaned['WAGE_UNIT_OF_PAY_1'].value_counts()# the wage information that we have available has different unit of pay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hCGTUT22Z6Bj"
   },
   "outputs": [],
   "source": [
    "# we convert the different units of pay to the type 'Year'\n",
    "cleaned['WAGES'] = np.where(cleaned['WAGE_UNIT_OF_PAY_1'] == 'Month',cleaned['WAGES'] * 12,cleaned['WAGES'])\n",
    "cleaned['WAGES'] = np.where(cleaned['WAGE_UNIT_OF_PAY_1'] == 'Hour',cleaned['WAGES'] * 2080,cleaned['WAGES']) # 2080=8 hours*5 days* 52 weeks\n",
    "cleaned['WAGES'] = np.where(cleaned['WAGE_UNIT_OF_PAY_1'] == 'Bi-Weekly',cleaned['WAGES'] *26,cleaned['WAGES'])\n",
    "cleaned['WAGES'] = np.where(cleaned['WAGE_UNIT_OF_PAY_1'] == 'Week',cleaned['WAGES'] * 52,cleaned['WAGES'])\n",
    "cleaned.drop(columns=['WAGE_RATE_OF_PAY_FROM_1','WAGE_UNIT_OF_PAY_1'],axis=1,inplace=True)#we can drop the columns as cleaning is complete\n",
    "cleaned.dropna(inplace=True)# We can check if we have null records using cleaned.info() and drop null records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M03_5gLDxgYT"
   },
   "outputs": [],
   "source": [
    "cleaned['JOB_TITLE']=cleaned.JOB_TITLE.apply(lambda txt: \" \".join([cw.remove_num(i) for i in txt.lower().split()]))\n",
    "cleaned['JOB_TITLE']=cleaned['JOB_TITLE'].str.replace(',', '')\n",
    "cleaned['SOC_TITLE']=cleaned.SOC_TITLE.apply(lambda txt: \" \".join([cw.remove_num(i) for i in txt.lower().split()]))\n",
    "cleaned['SOC_TITLE']=cleaned['SOC_TITLE'].str.replace(',', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3DxeRz6Mxkx1"
   },
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "words = set(nltk.corpus.words.words())\n",
    "spell = Speller()\n",
    "def lemmatize_text(text):\n",
    "     return lemmatizer.lemmatize(text)\n",
    "def spelling_checker(text):\n",
    "     return spell(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zMiGTMl414Mq"
   },
   "outputs": [],
   "source": [
    "cleaned['JOB_TITLE']=cleaned.JOB_TITLE.apply(lambda txt: \" \".join([lemmatize_text(i) for i in txt.lower().split()]))\n",
    "cleaned['JOB_TITLE']=cleaned.JOB_TITLE.apply(lambda txt: \" \".join([spelling_checker(i) for i in txt.lower().split()]))\n",
    "cleaned['SOC_TITLE']=cleaned.SOC_TITLE.apply(lambda txt: \" \".join([lemmatize_text(i) for i in txt.lower().split()]))\n",
    "cleaned['SOC_TITLE']=cleaned.SOC_TITLE.apply(lambda txt: \" \".join([spelling_checker(i) for i in txt.lower().split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iBMWRKQo14Mv"
   },
   "outputs": [],
   "source": [
    "cleaned = cleaned.groupby(\"SOC_CODE\").filter(lambda x: len(x) > 15)\n",
    "cleaned = cleaned.groupby(\"SOC_TITLE\").filter(lambda x: len(x) > 15)\n",
    "cleaned['EMPLOYER_NAME'].value_counts()\n",
    "cleaned = cleaned.groupby(\"EMPLOYER_NAME\").filter(lambda x: len(x) > 15)\n",
    "Top_Employer=cleaned['EMPLOYER_NAME'].value_counts()[:10]\n",
    "cleaned['WAGE_CATEGORY'] = cleaned['WAGES'].apply(remove.wage_feature_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "colab_type": "code",
    "id": "IARZcE10wHBl",
    "outputId": "fba86b51-0e22-4bae-8efc-bc15c850e977"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8,8])\n",
    "ax=sns.barplot(y=Top_Employer.index,x=Top_Employer.values,palette=sns.color_palette('viridis',10))\n",
    "ax.tick_params(labelsize=12)\n",
    "for i, v in enumerate(Top_Employer.values): \n",
    "    ax.text(.5, i, v,fontsize=15,color='white',weight='bold')\n",
    "plt.title('Top 10 Companies sponsoring H1B Visa in 2019', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pdms5n6Fz7SO"
   },
   "outputs": [],
   "source": [
    "state={\"AL\":\"ALABAMA\",\"AK\":\"ALASKA\",\"AZ\":\"ARIZONA\",\"AR\":\"ARKANSAS\",\"CA\":\"CALIFORNIA\",\"CO\":\"COLORADO\",\"DE\":\"DELAWARE\",\\\n",
    "       \"FL\":\"FLORIDA\",\"GA\":\"GEORGIA\",\"HI\":\"HAWAII\",\"ID\":\"IDAHO\",\"IL\":\"ILLINOIS\",\"IN\":\"INDIANA\",\"IA\":\"IOWA\",\"KS\":\"KANSAS\",\\\n",
    "       \"KY\":\"KENTUCKY\",\"LA\":\"LOUISIANA\",\"ME\":\"MAINE\",\"MD\":\"MARYLAND\",\"MA\":\"MASSACHUSETTS\",\"MI\":\"MICHIGAN\",\"MN\":\"MINNESOTA\",\\\n",
    "       \"MS\":\"MISSISSIPPI\",\"MO\":\"MISSOURI\",\"MT\":\"MONTANA\",\"NE\":\"NEBRASKA\",\"NV\":\"NEVADA\",\"NH\":\"NEW HAMPSHIRE\",\"NJ\":\"NEW JERSEY\",\\\n",
    "       \"NM\":\"NEW MEXICO\",\"NY\":\"NEW YORK\",\"NC\":\"NORTH CAROLINA\",\"ND\":\"NORTH DAKOTA\",\"OH\":\"OHIO\",\"OK\":\"OKLAHOMA\",\"OR\":\"OREGON\",\\\n",
    "       \"PA\":\"PENNSYLVANIA\",\"RI\":\"RHODE ISLAND\",\"SC\":\"SOUTH CAROLINA\",\"SD\":\"SOUTH DAKOTA\",\"TN\":\"TENNESSEE\",\"TX\":\"TEXAS\",\\\n",
    "       \"UT\":\"UTAH\",\"VT\":\"VERMONT\",\"VA\":\"VIRGINIA\",\"WA\":\"WASHINGTON\",\"WV\":\"WEST VIRGINIA\",\"WI\":\"WISCONSIN\",\"WY\":\"WYOMING\",\\\n",
    "       \"PR\":\"PUERTO RICO\",\"VI\":\"U.S. VIRGIN ISLANDS\",\"MP\":\"NORTHERN MARIANA ISLANDS\",\"GU\":\"GUAM\",\"MH\":\"MARSHALL ISLANDS\",\\\n",
    "       \"PW\":\"PALAU\",\"DC\":\"DISTRICT OF COLUMBIA\",\"CT\":\"CONNECTICUT\"}\n",
    "cleaned.replace({\"WORKSITE_STATE_1\": state})\n",
    "cleaned = cleaned.groupby(\"WORKSITE_STATE_1\").filter(lambda x: len(x) > 15) #removing less significant records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wIUdVPMR14Nj"
   },
   "outputs": [],
   "source": [
    "#CONVERTING CATEGORICAL COLUMNS INTO NUMERIC COLUMNS\n",
    "cleaned.loc[(cleaned.CASE_STATUS == \"CERTIFIED\"),\"CASE_STATUS\"] = 1\n",
    "cleaned.loc[(cleaned.CASE_STATUS == \"DENIED\"),\"CASE_STATUS\"] = 0\n",
    "cleaned.loc[(cleaned.FULL_TIME_POSITION == \"Y\"),\"FULL_TIME_POSITION\"] = 1\n",
    "cleaned.loc[(cleaned.FULL_TIME_POSITION == \"N\"),\"FULL_TIME_POSITION\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lw2l9vMB14Nt"
   },
   "source": [
    "### Baseline classifier\n",
    "\n",
    "The baseline classifier is done with a basic model. In this case we are taking the mean of the labels ('certified' and 'denied' for H1B visa approvals). It will give us the base accuracy to which we will compare our classifier's accuracy. Our classifier should have a better accuracy than the baseline classifier accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kBUpRntv14Nu"
   },
   "outputs": [],
   "source": [
    "# This step assigns a binary class label (0 or 1) to each label for H1B visa approval. \n",
    "def create_class_labels(processed_data):\n",
    "    y = np.where((processed_data['CASE_STATUS']=='CERTIFIED'),1, 0)\n",
    "    return y\n",
    "X = cleaned['CASE_STATUS'].to_numpy()\n",
    "y = create_class_labels(cleaned)# Groundtruth labels for the dataset\n",
    "counts = cleaned['CASE_STATUS'].value_counts()\n",
    "print(counts)\n",
    "print('proportion: ', counts[0]/counts[1], ': 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c_XrA5rj14Nx"
   },
   "outputs": [],
   "source": [
    "class BaselineClasifier(): # Baseline classifier that predicts the class base on the mode of the labels.\n",
    "    def __init__(self):\n",
    "        self.central_tendency = None\n",
    "    def fit(self, data, y, central_t='mode'): \n",
    "        label, counts = np.unique(y, return_counts=True) # Count labels and find the most frequent one \n",
    "        if central_t == 'mode':\n",
    "            self.central_tendency = counts.argmax()\n",
    "        elif central_t == 'mean':\n",
    "            self.central_tendency = round(np.sum(y)/len(y))\n",
    "        return self# Return an array with size equal to the data size  and each element setted to the mode.\n",
    "    def predict(self, data):\n",
    "        result = np.full(data.shape[0], self.central_tendency)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iwmIZYCt14Nz"
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(validation, predicted):\n",
    "    comp = prediction == validation \n",
    "    match_counts = np.count_nonzero(comp == True) \n",
    "    clasifier_accuracy = match_counts/len(validation)\n",
    "    return clasifier_accuracy  \n",
    "def compute_AUC(y, prediction):\n",
    "    auc = roc_auc_score(y, prediction)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1gr9AgXB14N4"
   },
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "kf = sklearn.model_selection.KFold(n_splits=4, random_state=1, shuffle=True)# Testing with K-folds \n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx] \n",
    "    baseline_clasifier = BaselineClasifier()\n",
    "    classifier = baseline_clasifier.fit(X_train, y_train, 'mean')\n",
    "    prediction = baseline_clasifier.predict(X_test)\n",
    "    fold_accuracy = compute_accuracy(y_test, prediction)\n",
    "    fold_AUC = compute_AUC(y_test, prediction)\n",
    "    accuracies.append(fold_accuracy)\n",
    "baseline_clasifier_accuracy = mean(accuracies)\n",
    "print('Baseline accuracy: ', baseline_clasifier_accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2WJnr6p414N7"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)# Testing with regular split\n",
    "baseline_clasifier = BaselineClasifier()\n",
    "classifier = baseline_clasifier.fit(X_train, y_train, 'mean')\n",
    "prediction = baseline_clasifier.predict(X_test)\n",
    "split_accuracy = compute_accuracy(y_test, prediction)\n",
    "split_AUC = compute_AUC(y_test, prediction)\n",
    "print('Baseline accuracy: ', split_accuracy)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oyr0Q6LN14OA"
   },
   "source": [
    "The accuracy results of the baseline classifier is 0.99. This result is due to the highly imbalanced data, where there are 624682 CERTIFIED applications and 5158 DENIED applications. The proportion of it is 121.10934470725087 to 1. Therefore, a performance measure based on the accuracy is not a good one. A better performance measure in imbalanced data is the Area under the ROC Curve (AUC). It meassures the likelihood that given two random points (one from the positive and one from the negative class) the classifier will rank the point from the positive class higher than the one from the negative one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "me1Gx1mr14OO"
   },
   "outputs": [],
   "source": [
    "Dataset = cleaned[[\"CASE_STATUS\", \"FULL_TIME_POSITION\",\"JOB_TITLE\", \"SOC_CODE\", \"SOC_TITLE\", \"EMPLOYER_NAME\", \"WORKSITE_STATE_1\", \"WAGE_CATEGORY\"]]\n",
    "Top_Job_positions = Dataset[\"JOB_TITLE\"].value_counts().head(72)\n",
    "def job_function(job):#Considered top positions in job roles\n",
    "    if job in Top_Job_positions:\n",
    "        return job\n",
    "    else:\n",
    "        return \"others\"\n",
    "Dataset[\"JOB_POSITION\"] = Dataset[\"JOB_TITLE\"].apply(job_function)\n",
    "Dataset[\"JOB_POSITION\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O6a2puyU14OS"
   },
   "outputs": [],
   "source": [
    "Top_SOC_CODES = Dataset[\"SOC_CODE\"].value_counts().head(70)#Considered top domains\n",
    "def soc_function(soc):\n",
    "    if soc in Top_SOC_CODES:\n",
    "        return soc\n",
    "    else:\n",
    "        return \"others\"\n",
    "Dataset[\"TOP_SOC_CODE\"] = Dataset[\"SOC_CODE\"].apply(soc_function)\n",
    "Dataset[\"TOP_SOC_CODE\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c97FqVaK14Ob"
   },
   "outputs": [],
   "source": [
    "Top_SOC_TITLE = Dataset[\"SOC_TITLE\"].value_counts().head(70)#Considered names of top domains\n",
    "def soc_function(soc):\n",
    "    if soc in Top_SOC_TITLE:\n",
    "        return soc\n",
    "    else:\n",
    "        return \"others\"\n",
    "Dataset[\"TOP_SOC_TITLE\"] = Dataset[\"SOC_TITLE\"].apply(soc_function)\n",
    "Dataset[\"TOP_SOC_TITLE\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VzvrTbeW14Of"
   },
   "outputs": [],
   "source": [
    "Top_EMPLOYER = Dataset[\"EMPLOYER_NAME\"].value_counts().head(70)#Considered top employers\n",
    "def emp_function(emp):\n",
    "    if emp in Top_EMPLOYER:\n",
    "        return emp\n",
    "    else:\n",
    "        return \"others\"\n",
    "Dataset[\"TOP_EMPLOYER\"] = Dataset[\"EMPLOYER_NAME\"].apply(emp_function)\n",
    "Dataset[\"TOP_EMPLOYER\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JdKSXCt714Ol"
   },
   "outputs": [],
   "source": [
    "Dataset.drop(columns=['JOB_TITLE','SOC_CODE','SOC_TITLE','EMPLOYER_NAME',],axis=1,inplace=True)\n",
    "Encoding = OneHotEncoder(handle_unknown='ignore',sparse = True)\n",
    "Encoding_df = pd.DataFrame(SOC_Encoding.fit_transform(Dataset[[\"WORKSITE_STATE_1\",\"JOB_POSITION\",\"TOP_SOC_CODE\",\"TOP_SOC_TITLE\",\"TOP_EMPLOYER\"]]).toarray())\n",
    "Dataset = Dataset.join(Encoding_df)\n",
    "Dataset.drop(columns=['WORKSITE_STATE_1','JOB_POSITION','TOP_SOC_CODE','TOP_SOC_TITLE','TOP_EMPLOYER','WAGE_CATEGORY'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wtqqupdU14O1"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=20)#Split the date into training and test set\n",
    "model_clf = RandomForestClassifier(n_jobs=2,random_state=0)\n",
    "model_clf.fit(X_train,y_train)#train the model\n",
    "prediction_test = model_clf.predict(X_test)#test the model (predict with our test data)\n",
    "print(\"Accuracy = \", metrics.accuracy_score(y_test, prediction_test))#compare with original value, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D4886emk14PA",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Test a simple input to predict its STATUS.\n",
    "status_label = np.array(['Denied','Approved'])\n",
    "new_pred_number = model_clf.predict([\n",
    "    [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\\\n",
    "     0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\\\n",
    "     0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]])\n",
    "new_pred_label= status_label[ new_pred_number ]\n",
    "new_pred_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HMPmHHbo14PE"
   },
   "source": [
    "### Reflection\n",
    "\n",
    "What is the hardest part of the project that youâ€™ve encountered so far?\n",
    "\n",
    "1. Setting up the data for visualization and ML analysis, e.g. same job title is cluttered with different words, integers, and punctuation characters. \n",
    "2. Encoding the dataset to be used in the Classifier. We tried with JOB_TITLE attribute but got Memory error, instead, we started to use SOC_TITLE attribute"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "H1b_project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
