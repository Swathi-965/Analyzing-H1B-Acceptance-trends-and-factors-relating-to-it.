{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "H1B_Visa_Data_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-S6Q8i5yQ_zF"
      },
      "source": [
        "## Analysing H1B Visa data Trends"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G3DkkMAuRQ9j"
      },
      "source": [
        "H1B visa is a nonimmigrant visa issued to gradaute level applicants allowing them to work in the United States. The employer sponsors the H1B visa for workers with theoretical or technical expertise in specialized fields such as in IT, finance, accounting etc. An interesting fact about immigrant workers is that about 52 percent of new Silicon valley companies were founded by such workers during 1995 and 2005. Some famous CEOs like Indira Nooyi (Pepsico), Elon Musk (Tesla), Sundar Pichai (Google),Satya Nadella (Microsoft) once arrived to the US on a H1B visa.\\\n",
        "**Motivation**: Our team consists of five international graduate students, in the future we will be applying for H1B visa. The visa application process seems very long, complicated and uncertain. So we decided to understand this process and use Machine learning algorithms to predict the acceptance rate and trends of H1B visa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w978Te7MRoFx",
        "colab": {}
      },
      "source": [
        "from google.colab import drive # mount google drive to access the datasets \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lmhgvIddSRf0",
        "colab": {}
      },
      "source": [
        "!pip install autocorrect\n",
        "import string,nltk,warnings,sklearn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import read_files as rd\n",
        "import visualizations as vz\n",
        "import plotly.graph_objects as go\n",
        "import plotly.tools as tls\n",
        "import cleaning as cw\n",
        "import baseline as blc\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statistics import mean\n",
        "from autocorrect import Speller \n",
        "from imblearn.over_sampling import ADASYN\n",
        "from imblearn.under_sampling import EditedNearestNeighbours\n",
        "from collections import Counter\n",
        "from sklearn import metrics\n",
        "from sklearn import model_selection\n",
        "from sklearn.preprocessing import OneHotEncoder #ONE HOT ENCODING\n",
        "from sklearn.ensemble import RandomForestClassifier #Build model - Random Forest Classifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import gensim.downloader as api\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "nltk.download('wordnet')\n",
        "nltk.download('words')\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ap8NQkonSOJq"
      },
      "source": [
        "### Data \n",
        "The data used in the project has been collected from <a href=\"https://www.foreignlaborcert.doleta.gov/performancedata.cfm\">the Office of Foreign Labor Certification (OFLC).</a>The Data provides insight into each petition with information such as the Job title, Wage, Employer, Worksite location etc. To get the dataset click on the above link-> click on Disclosure data -> scroll down to H1B data.\\\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNqVwWQdWfhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link2015='/content/gdrive/My Drive/H1B_project/H-1B_Disclosure_Data_FY15_Q4.csv'\n",
        "link2016='/content/gdrive/My Drive/H1B_project/H-1B_Disclosure_Data_FY16.csv'\n",
        "link2017='/content/gdrive/My Drive/H1B_project/H-1B_Disclosure_Data_FY17.csv'\n",
        "link2018='/content/gdrive/My Drive/H1B_project/H-1B_Disclosure_Data_FY2018_EOY.csv'\n",
        "link2019='/content/gdrive/My Drive/H1B_project/H-1B_Disclosure_Data_FY2019.csv'\n",
        "#read csv fuction to read and concatenate all the datasets\n",
        "cleaned=rd.read_csv(pd,link2015,link2016,link2017,link2018,link2019)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qCok1RY0VFeM",
        "colab": {}
      },
      "source": [
        "original=cleaned.copy()\n",
        "cleaned.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S2RKhcbJycxS"
      },
      "source": [
        "### Exploratory Data Analysis\n",
        "Before we begin working on our data we need to understand the traits of our data which we accomplish using EDA. We see that we have about 260 columns , not all 260 columns have essential information that contributes to our analysis. Hence we pick out the columns such as case status( Accepted/ Denied) ,Employer, Job title etc.We can look at all the colums and the types of object in each column using cleaned.info(). We also use cleaned['column_name'].value_counts() to find the classes in the column along with the count. For feature engineering we are converting quantitative data to categorical.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jEaANag2w5Uk"
      },
      "source": [
        "####Granularity:\n",
        "Each row in the dataframe represents an application that was filled by the employer and some values are added during the processing of the application such as DECISION_DATE, CASE_STATUS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YI8muywco9DA",
        "colab": {}
      },
      "source": [
        "cleaned.dropna(inplace=True)\n",
        "cleaned.shape#shape of the df after dropping all null values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oG0hPOcJI78r",
        "colab": {}
      },
      "source": [
        "cleaned.head(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y5Pbv_3_mkH_",
        "colab": {}
      },
      "source": [
        "cleaned['VISA_CLASS'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k37klRspm4BW",
        "colab": {}
      },
      "source": [
        "# the dataset has categories for Visa which are not of use , we require only H1B visa type , hence we drop all records with other visa types\n",
        "cleaned.drop(labels=cleaned.loc[cleaned['VISA_CLASS']!='H-1B'].index , inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "efuJruAvnNMV",
        "colab": {}
      },
      "source": [
        "cleaned['CASE_STATUS'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4GXmkkTrnBaI",
        "colab": {}
      },
      "source": [
        "#In case status column we can drop withdrawn records and certified-withdrawn records\n",
        "cleaned.drop(labels=cleaned.loc[cleaned['CASE_STATUS']=='WITHDRAWN'].index , inplace=True)\n",
        "cleaned.drop(labels=cleaned.loc[cleaned['CASE_STATUS']=='CERTIFIED-WITHDRAWN'].index , inplace=True)\n",
        "cleaned['CASE_STATUS'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3aOpaG2Dohr7",
        "colab": {}
      },
      "source": [
        "cleaned.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MJ-yRmY0ehSJ"
      },
      "source": [
        "####Temporality\n",
        "The data is available from 9/9/2019 to 1/1/2015"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M_EkmaiQegGn",
        "colab": {}
      },
      "source": [
        "max_date=cleaned.CASE_SUBMITTED.max()\t\n",
        "min_date=cleaned.CASE_SUBMITTED.min()\n",
        "print(\"The date in the dataset ranges from \"+str(max_date)+\" to\",min_date)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nbfyEEmngLY2",
        "colab": {}
      },
      "source": [
        "#the column wages has a mix of both string and float value types and some record have the symbol '$' which we want to remove\n",
        "cleaned['WAGES']=cleaned['WAGE_RATE_OF_PAY_FROM'].apply(cw.clean_wages).astype('float')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VDi3g4fIgRDm",
        "colab": {}
      },
      "source": [
        "# the wage information that we have available has different unit of pay\n",
        "cleaned['WAGE_UNIT_OF_PAY'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VBCLazyqgXxu",
        "colab": {}
      },
      "source": [
        "# we convert the different units of pay to the type 'Year'\n",
        "cleaned= cw.clean_wageUnit(np,cleaned)\n",
        "cleaned.drop(columns=['WAGE_RATE_OF_PAY_FROM','WAGE_UNIT_OF_PAY'],axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vhMx0hpTjlRR",
        "colab": {}
      },
      "source": [
        "#checking if the WAGES column have outliers\n",
        "print(cleaned['WAGES'].describe())\n",
        "sns.boxplot(y=cleaned['WAGES'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UEGwyt3ukJvG"
      },
      "source": [
        "We can see that we have large outliers , we can remove them from the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2uWva-1ulVgx",
        "colab": {}
      },
      "source": [
        "# Remove the outliers.\n",
        "lowerBound = 0.001\n",
        "upperBound = 0.95\n",
        "result = cleaned.WAGES.quantile([lowerBound, upperBound])\n",
        "cleaned = cleaned[(result.loc[lowerBound] < cleaned.WAGES.values) &\\\n",
        "                  (cleaned.WAGES.values < result.loc[upperBound])]\n",
        "print(cleaned['WAGES'].describe())\n",
        "sns.boxplot(y=cleaned['WAGES'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vJ-OdEdRp_TU",
        "colab": {}
      },
      "source": [
        "#Plot for top 10 companies that have the most applications for H1B visa\n",
        "plt=vz.top_employer(plt,sns,cleaned)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "30UMuuYNrzJT",
        "colab": {}
      },
      "source": [
        "cleaned= cw.clean_states(cleaned)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6y8zxWPftW6U"
      },
      "source": [
        "#### Data Scope\n",
        "The data available contains applications submitted by Employers from 2015 till 2019. \\\n",
        "Geographical Scope: The WORKSITE_STATE gives information about the state where the primary worksite location. The data set covers all the states of US. \n",
        "The Data set also cover different Industries majority of the applications are for IT industry but not limited to IT, it also covers Medical, Pharmacy, Law, Carpentery etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KcVj-dd2vjUC",
        "colab": {}
      },
      "source": [
        "df=cleaned.SOC_TITLE.value_counts()\n",
        "df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XljvOXxQscAQ",
        "colab": {}
      },
      "source": [
        "df=cleaned.WORKSITE_STATE.value_counts()\n",
        "df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bwN87sFixRWt"
      },
      "source": [
        "####Faithfulness:\n",
        "Each record in the dataframe represents an application filled by the employer, hence we see a lot of spelling mistakes and some punctutaions like ,(comma) and some numbers in the JOB_TITLE, SOC_TITLE feilds, but there are no inconsistencies and data falsification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jlzTPhST7G0B",
        "colab": {}
      },
      "source": [
        "#The Job_title and Soc_title has inconsistencies such as numeric characters and symbols so it has to be cleaned\n",
        "cleaned['JOB_TITLE']=cleaned.JOB_TITLE.apply(lambda txt: \" \".join([cw.remove_num(i) for i in txt.lower().split()]))\n",
        "cleaned['JOB_TITLE']=cleaned['JOB_TITLE'].str.replace(',', '')\n",
        "cleaned['SOC_TITLE']=cleaned.SOC_TITLE.apply(lambda txt: \" \".join([cw.remove_num(i) for i in txt.lower().split()]))\n",
        "cleaned['SOC_TITLE']=cleaned['SOC_TITLE'].str.replace(',', '')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jHWSBSgF7pXX",
        "colab": {}
      },
      "source": [
        "#Function to clean data columns, to correct spelling mistakes and convert plural words to singular\n",
        "cleaned=cw.text_clean(cleaned) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SepT8DPRk7nJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned.SOC_TITLE.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ssEwlv1n7xo1"
      },
      "source": [
        "The has lot of different subcategories which have less than 15 applications in them which does not show much significance , removing them help us condense the data and take only the significant rows.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GoOYWV-bCabm",
        "colab": {}
      },
      "source": [
        "cleaned= cw.drop_less_significant(cleaned)\n",
        "cleaned=cw.cat_to_num(cleaned)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uPEVdBTo1ZC3",
        "colab": {}
      },
      "source": [
        "data_scnt,data_anlst,data_eng,mach_learn=cw.data_jobs(cleaned)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5eM8UzR-17Og"
      },
      "source": [
        "## Are you a international student? Are you looking for jobs in Data Science domain?\n",
        "The below visualization shows median salary for four data science related jobs and the number of visa applications that are submitted for each job. This gives us insights about the jobs available for international students/workers about the job market for Data Science domain. We get to know the number of jobs available for each caegory as well as the median wage for each category. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QH1xs65O2Dp3",
        "colab": {}
      },
      "source": [
        "f,ax=plt.subplots(figsize=(8,6))\n",
        "bplot1=plt.boxplot([data_scnt[data_scnt['WAGES']<200000].WAGES,data_anlst[data_anlst['WAGES']<200000].WAGES,data_eng[data_eng['WAGES']<200000].WAGES,mach_learn[mach_learn['WAGES']<200000].WAGES],patch_artist=\"True\")\n",
        "ax.set_xticklabels(['Data Scientists','Data Analysts','Data Engineer','Machine Learning'],fontsize=15)\n",
        "ax.set_title('Salary Distribution for jobs in Data Science field in 2019', fontsize=15)\n",
        "ax.tick_params(labelsize=10)\n",
        "colors = ['blue','orange', 'green', 'red'] \n",
        "for patch, color in zip(bplot1['boxes'], colors): \n",
        "  patch.set_facecolor(color)\n",
        "plt.show()\n",
        "datajobs=cw.data_concat(pd,data_scnt,data_anlst,data_eng,mach_learn)\n",
        "ax2=sns.countplot(x=\"data\", data=datajobs)\n",
        "ax2.set_title('Number of petitions for each jobs in Data Science field in 2019', fontsize=15)\n",
        "warnings.simplefilter(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69dcI-dqZ5_2",
        "colab_type": "text"
      },
      "source": [
        "##Job Distribustion in the US for H1b applicants.\n",
        "The below visualizations gives us insights into the jobs available in USA for every state. H1B applicants can aim for jobs in states with high job openings.As the majority of applicants are from IT domain and software domain we can see that California has the highest number of jobs.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUVQRgOBbLlT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned=cw.states_vis(cleaned)\n",
        "df=cleaned.groupby(['CODE']).size().reset_index(name='counts')\n",
        "fig=vz.USA_map(go,tls,df)\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBH7kYOucHO4",
        "colab_type": "text"
      },
      "source": [
        "### Comparison of Education levels for some common courses\n",
        "![picture](https://drive.google.com/uc?id=1J2SP3r-B17JZSRwkCE34O87iBBqx5CMc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7H3wXPnEnse3"
      },
      "source": [
        "**Features considered for the classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mNftZfY8lCCE",
        "colab": {}
      },
      "source": [
        "Dataset = cleaned[[\"CASE_STATUS\", \"FULL_TIME_POSITION\",\"SOC_TITLE\", \"WORKSITE_STATE\", \"WAGES\", \"H1B_DEPENDENT\"]]\n",
        "Dataset.reset_index(drop = True,inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fJqE8futomje"
      },
      "source": [
        "**CONVERTING CATEGORICAL COLUMNS TO NUMERIC COLUMNS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r5QiHud8lUTI",
        "colab": {}
      },
      "source": [
        "# Cleaning SOC_TITLE(job_positions) column by removing punctuations\n",
        "Dataset=cw.fun_punctuation(Dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fg6RHFPqpD4k"
      },
      "source": [
        "**If we have more number of unique values in the categorical columns we can use word_vectors to convert them into numeric columns as one-hot encoding fails to convert.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pzg1qFcIl2Jh",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(columns = [\"job_position\"])\n",
        "Dataset = cw.fun_word_vectors(Dataset)# converting job titles to numeric values using word_vectors.\n",
        "df[\"job_position\"] = Dataset[\"SOC_TITLE\"]\n",
        "df[\"job_position\"] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qfGdfLUTmDBK",
        "colab": {}
      },
      "source": [
        "Dataset.drop(columns = [\"SOC_TITLE\"],axis=1,inplace=True)\n",
        "Dataset.reset_index(drop = True, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tdPXmezNmKZM",
        "colab": {}
      },
      "source": [
        "# one hot encoding to convert states column to numeric values using one hot encoding\n",
        "Encoding = OneHotEncoder(handle_unknown = 'ignore',sparse = True)\n",
        "Encoding_df = pd.DataFrame(Encoding.fit_transform(Dataset[[\"WORKSITE_STATE\"]]).toarray())\n",
        "Dataset = Dataset.join(Encoding_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qkZK5mdxma5U",
        "colab": {}
      },
      "source": [
        "Dataset.drop(columns = ['WORKSITE_STATE'],axis=1,inplace=True)\n",
        "word_vectors = df[[\"job_position\"]].values\n",
        "temp = np.vstack(word_vectors.tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xk1xerqrquLB",
        "colab": {}
      },
      "source": [
        "# Y represents the output labels\n",
        "Y = Dataset[\"CASE_STATUS\"].astype(float)\n",
        "Dataset.drop(columns = [\"CASE_STATUS\"],inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xGHyM_iTnEiD",
        "colab": {}
      },
      "source": [
        "# all_features represent the input labels\n",
        "other_features = Dataset.iloc[:].values\n",
        "all_features = np.hstack([temp, other_features])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tSqJobhYOj-0",
        "colab": {}
      },
      "source": [
        "#Dependent and independent variables\n",
        "X = all_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "52vYSJW8FjF5"
      },
      "source": [
        "\n",
        "### Baseline classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-lLzCf96GZiD"
      },
      "source": [
        "The baseline classifier is done with a basic model. In this case we are taking the mean of the labels ('certified' and 'denied' for H1B visa approvals). It will give us the base accuracy to which we will compare our classifier's accuracy. Our classifier should have a better accuracy than the baseline classifier accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cCDCRjAqF1TB",
        "colab": {}
      },
      "source": [
        "# This step assigns a binary class label (0 or 1) to each label for H1B visa approval. \n",
        "y = Y.to_numpy().astype(int) # Groundtruth labels for the dataset\n",
        "counts = cleaned['CASE_STATUS'].value_counts()\n",
        "print(counts)\n",
        "print('proportion: ', counts[1]/counts[0], ': 1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EBm_dFDZF8LD",
        "colab": {}
      },
      "source": [
        "accuracies = []\n",
        "AUCs = []\n",
        "kf = sklearn.model_selection.KFold(n_splits=4, random_state=1, shuffle=True)# Testing with K-folds \n",
        "for train_idx, test_idx in kf.split(X):\n",
        "    X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx] \n",
        "    prediction = blc.run_clasifier(X_train, y_train, X_test, np)\n",
        "    fold_accuracy = blc.compute_accuracy(y_test, prediction)\n",
        "    fold_AUC = blc.compute_AUC(y_test, prediction)\n",
        "    accuracies.append(fold_accuracy)\n",
        "    if fold_AUC != None: AUCs.append(fold_AUC)\n",
        "baseline_clasifier_accuracy = mean(accuracies)\n",
        "print('Baseline accuracy (K-fold): ', baseline_clasifier_accuracy)\n",
        "print('AUC K-fold: ', mean(AUCs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O1BXDP3TGD8-"
      },
      "source": [
        "The initial accuracy results of the baseline classifier was 0.99. This result is due to the highly imbalanced data with respect to the Case Status, where there are 624682 CERTIFIED applications and 5158 DENIED applications. The proportion of it is 134.02 to 1. Therefore, a performance measure based on the accuracy is not a good one. A better performance measure in imbalanced data is the Area under the ROC Curve (AUC). It meassures the likelihood that given two random points (one from the positive and one from the negative class) the classifier will rank the point from the positive class higher than the one from the negative one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YRiXS6aKyKz3"
      },
      "source": [
        "### Balancing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M8Mq8K89lpqz"
      },
      "source": [
        "Given the inbalanced data we have, we lack observations required to train an effective model that classifies the status of the H1B visa. To addess that, a good idea is to upsample the minority class and undersample the majority class. First, the ADASYN algorithm is applied to upsample the minority class. It generates new related samples and some random samples to avoid linear correlation to the parent. Second, the ENN algorithm is applied to undersample the majority class. It focuses on removing the noisy observations. The expected proportion between the two classes is 1 to 2 at most."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qybksxpSqxFn",
        "colab": {}
      },
      "source": [
        "# Oversampling the minority class\n",
        "ada = ADASYN(sampling_strategy=0.5, random_state=0)\n",
        "X_ada, y_ada = ada.fit_resample(X, Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-OOFug6uq04x",
        "colab": {}
      },
      "source": [
        "# Undersampling the majority class\n",
        "enn = EditedNearestNeighbours()\n",
        "X, Y = enn.fit_resample(X_ada, y_ada)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sweKaPRZq5eL",
        "colab": {}
      },
      "source": [
        "count_y_ada_enn=Counter(Y)\n",
        "print('Balanced dataset shape %s' % count_y_ada_enn)\n",
        "print('proportion: ', count_y_ada_enn[1]/count_y_ada_enn[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YpX2h3fhF_Du",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25)# Testing with regular split\n",
        "prediction_bl = blc.run_clasifier(X_train, y_train, X_test, np)\n",
        "split_accuracy = blc.compute_accuracy(y_test, prediction_bl)\n",
        "split_AUC = blc.compute_AUC(y_test, prediction_bl)\n",
        "print('Baseline accuracy (split): ', split_accuracy)  \n",
        "print('AUC split (80-20): ', split_AUC)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h39Oi-z5rjOZ",
        "colab": {}
      },
      "source": [
        "# Confusion matrix\n",
        "tn, fp, fn, tp = metrics.confusion_matrix(y_test, prediction_bl).ravel()\n",
        "print(\"tn, fp, fn, tp\", (tn, fp, fn, tp))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a0P24Xnlr40h",
        "colab": {}
      },
      "source": [
        "# classification report\n",
        "report = metrics.classification_report(y_test, prediction_bl)\n",
        "print(report)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8-xGF0ehsTaz",
        "colab": {}
      },
      "source": [
        "# Plot precision recall curve\n",
        "plt=vz.plot_precision_recall(plt,y_test, prediction_bl,precision_recall_curve)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "59v_tJlThq6d"
      },
      "source": [
        "### Random forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zrMgSvnoPMN6",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=20)\n",
        "model_clf=RandomForestClassifier(n_jobs=2,random_state=0)\n",
        "model_clf.fit(X_train,y_train)#train the model\n",
        "prediction_test = model_clf.predict(X_test)#test the model (predict with our test data)\n",
        "prediction_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k9xAQ6fCPict",
        "colab": {}
      },
      "source": [
        "#compare with original value, Y_test\n",
        "print(\"Accuracy = \", metrics.accuracy_score(y_test, prediction_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nsJPocIrrn27",
        "colab": {}
      },
      "source": [
        "# Confusion matrix\n",
        "metrics.confusion_matrix(y_test, prediction_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fqxrtvO_rn29",
        "colab": {}
      },
      "source": [
        "# classification report\n",
        "report = metrics.classification_report(y_test, prediction_test)\n",
        "report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GpbY4QVSsgv5",
        "colab": {}
      },
      "source": [
        "y_probs_rand_forest = model_clf.predict_proba(X_test)\n",
        "vz.plot_precision_recall(plt, y_test, y_probs_rand_forest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z0ZnYIKr98kv"
      },
      "source": [
        "##### **President Trump's \"Buy American and Hire American\" Executive order's impact on H1B visa**\n",
        "The \"Buy American and Hire American\" order was passed on April 18, 2017, to Order to protect American workers and strengthen the American economy. Among other purposes, it aims to detect and prevent fraud in the process. Also, to avoid the abuse of the program which is heavily used by technology companies. This statement is interesting to evaluate from our perspective as international students.\n",
        "The after-effects of which caused a gradual decrease in the request and acceptance of H1B visas have been creating more job opportunities for immigrants which in turn contributes to the American workforce.\n",
        "\n",
        "It was also observed that just before this order was passed there was a surge in the applications submitted. \n",
        "\n",
        "As the graph demonstrates, this type of order, after it has been approved, take some months (around 6 months in this case)  to show the after-effects. It's in 2018 when we see the number of applications and their acceptance is falling so far, as shown in the below visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q4W71q9s9xgb",
        "colab": {}
      },
      "source": [
        "plt=vz.plot_buy_american_order(plt, original, pd)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IXwKy7rB-i0V"
      },
      "source": [
        "From the above visualization made on a random sample of about 20000 records from 2015 to 2019, we can infer that the visa acceptance is gradually seen as a downfall, which is one the after-effects of the executive order passed earlier 2017.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eIo6zyaJsWly",
        "colab": {}
      },
      "source": [
        "#mapping wages to 4 categories\n",
        "cleaned['WAGE_CATEGORY'] =pd.cut(cleaned.WAGES,bins=[0,50000,100000,140000,200000],\n",
        "                                 labels=['LOW','AVERAGE','HIGH',\"VERY HIGH\"])\n",
        "df_temp = pd.DataFrame(columns = [\"WC_NUM\"])\n",
        "cleaned['WC_NUM'] = pd.cut(cleaned.WAGES,bins=[0,50000,100000,140000,200000],labels=[0,1,2,3])\n",
        "df_temp[\"WC_NUM\"]=pd.cut(cleaned.WAGES,bins=[0,50000,100000,140000,200000],labels=[0,1,2,3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN4oS2HT6USi",
        "colab_type": "text"
      },
      "source": [
        "### **WAGE RANGE RANGE PREDICTION:**\n",
        "Using the data from our dataset we divide the wages into 4 categories which is LOW, AVERAGE, HIGH, VERY_HIGH.\n",
        "And this predictor will predict the expected salary/wage range for any H1B visa holder or an H1B aspirant based on various factors like location , occupation ,employer etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF792HmH6UmO",
        "colab_type": "text"
      },
      "source": [
        "**BASELINE CLASSIFIER FOR WAGE RANGE PREDICTOR:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ra-Hst33s87V",
        "colab": {}
      },
      "source": [
        "#assigning X, y for baseline classifier wage classifier.\n",
        "X = cleaned['WC_NUM'].to_numpy()\n",
        "y = df_temp[\"WC_NUM\"].to_numpy().astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XTz__p7RtNe_",
        "colab": {}
      },
      "source": [
        "accuracies = []\n",
        "AUCs = []\n",
        "kf = sklearn.model_selection.KFold(n_splits=4, random_state=1, shuffle=True)# Testing with K-folds \n",
        "for train_idx, test_idx in kf.split(X):\n",
        "    X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx] \n",
        "    prediction = blc.run_clasifier(X_train, y_train, X_test, np)\n",
        "    fold_accuracy = blc.compute_accuracy(y_test, prediction)\n",
        "    fold_AUC = blc.compute_AUC(y_test, prediction)\n",
        "    accuracies.append(fold_accuracy)\n",
        "    if fold_AUC != None: AUCs.append(fold_AUC)\n",
        "baseline_clasifier_accuracy = mean(accuracies)\n",
        "print('Baseline accuracy (K-fold): ', baseline_clasifier_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7JK1GcwntT1Q",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)# Testing with regular split\n",
        "prediction = blc.run_clasifier(X_train, y_train, X_test, np)\n",
        "split_accuracy = blc.compute_accuracy(y_test, prediction)\n",
        "#split_AUC = multiclass_roc_auc_score(y_test, prediction)\n",
        "print('Baseline accuracy (split): ', split_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BxQHQdYEtbOV"
      },
      "source": [
        "**Features considered for the wage classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4XsLFYb0tlWG",
        "colab": {}
      },
      "source": [
        "order_date = '01-01-2018'\n",
        "cleaned['CASE_SUBMITTED']=pd.to_datetime(cleaned['CASE_SUBMITTED'])\n",
        "mask1 = (cleaned['CASE_SUBMITTED'] < order_date)\n",
        "cleaned_before = cleaned.loc[mask1]\n",
        "Dataset_wage=cleaned_before[[\"EMPLOYER_NAME\",\"FULL_TIME_POSITION\",\"SOC_TITLE\", \"WORKSITE_STATE\", \"WC_NUM\",\"CASE_STATUS\"]]\n",
        "Dataset_wage.reset_index(drop = True,inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4LAAiyi3rn3A",
        "colab": {}
      },
      "source": [
        "# Considering top 70 employers and remaining employers as others\n",
        "Top_EMPLOYER = Dataset_wage[\"EMPLOYER_NAME\"].value_counts().head(70)\n",
        "def emp_function(emp):\n",
        "    if emp in Top_EMPLOYER:\n",
        "        return emp\n",
        "    else:\n",
        "        return \"others\"\n",
        "Dataset_wage[\"TOP_EMPLOYER\"] = Dataset_wage[\"EMPLOYER_NAME\"].apply(emp_function)\n",
        "Dataset_wage.drop(columns=[\"EMPLOYER_NAME\"],axis = 1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O1FufvJDtujp",
        "colab": {}
      },
      "source": [
        "Dataset_wage[\"SOC_TITLE\"] = Dataset_wage[\"SOC_TITLE\"].apply(lambda x : cw.remove_punctuation(x))\n",
        "df_wage = pd.DataFrame(columns = [\"soc_title\"])\n",
        "Dataset_wage = cw.fun_word_vectors(Dataset_wage)# converting job titles to numeric values using word_vectors.\n",
        "df_wage[\"soc_title\"] = Dataset_wage[\"SOC_TITLE\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckiL_2Sxpnsw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_wage[\"soc_title\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j5EY_vJetvd6",
        "colab": {}
      },
      "source": [
        "Dataset_wage.drop(columns=[\"SOC_TITLE\"],axis=1,inplace=True)\n",
        "Dataset_wage.reset_index(drop = True, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "56YzptYyt5Q1",
        "colab": {}
      },
      "source": [
        "Encoding = OneHotEncoder(handle_unknown='ignore',sparse = True)\n",
        "Encoding_wage = pd.DataFrame(Encoding.fit_transform(Dataset_wage[[\"WORKSITE_STATE\",\"TOP_EMPLOYER\"]]).toarray())\n",
        "Dataset_wage = Dataset_wage.join(Encoding_wage)\n",
        "Dataset_wage.drop(columns=[\"WORKSITE_STATE\",\"TOP_EMPLOYER\"],axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tb3jtL7Ht8Vu",
        "colab": {}
      },
      "source": [
        "Dataset_wage.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WoiCFeD5uAiC",
        "colab": {}
      },
      "source": [
        "wv_wage = df_wage[[\"soc_title\"]].values\n",
        "temp_var = np.vstack(wv_wage.tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8lTfP36duC36",
        "colab": {}
      },
      "source": [
        "Y = Dataset_wage[\"WC_NUM\"].astype(float)\n",
        "Dataset_wage.drop(columns = [\"WC_NUM\"],inplace = True)\n",
        "# sum_features represent the input data \n",
        "other_features = Dataset_wage.iloc[:].values\n",
        "del Dataset_wage\n",
        "other_features.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZFgOr68HuIxP",
        "colab": {}
      },
      "source": [
        "sum_features = np.hstack([temp_var, other_features])\n",
        "X = sum_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wHaDpWvwu_hY",
        "colab": {}
      },
      "source": [
        "#Split the date into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X9sthvgPuMji",
        "colab": {}
      },
      "source": [
        "dec_clf = DecisionTreeClassifier(max_depth=15,criterion='gini')\n",
        "dec_clf.fit(X_train,y_train)#train the model\n",
        "prediction_test = dec_clf.predict(X_test)#test the model (predict with our test data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GeprScgguXFE",
        "colab": {}
      },
      "source": [
        "print(\"Accuracy = \", metrics.accuracy_score(y_test, prediction_test))#compare with original value, Y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5ZGgDm8buaNQ",
        "colab": {}
      },
      "source": [
        "metrics.confusion_matrix(y_test, prediction_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VRXhig74ugPZ",
        "colab": {}
      },
      "source": [
        "report = metrics.classification_report(y_test, prediction_test)\n",
        "print(report)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}