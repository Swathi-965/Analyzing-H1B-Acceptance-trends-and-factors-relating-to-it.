{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "H1B_Visa_Data_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-S6Q8i5yQ_zF"
      },
      "source": [
        "## Analysing H1B Visa data Trends"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G3DkkMAuRQ9j"
      },
      "source": [
        "H1B visa is a nonimmigrant visa issued to gradaute level applicants allowing them to work in the United States. The employer sponsors the H1B visa for workers with theoretical or technical expertise in specialized fields such as in IT, finance, accounting etc. An interesting fact about immigrant workers is that about 52 percent of new Silicon valley companies were founded by such workers during 1995 and 2005. Some famous CEOs like Indira Nooyi (Pepsico), Elon Musk (Tesla), Sundar Pichai (Google),Satya Nadella (Microsoft) once arrived to the US on a H1B visa.\\\n",
        "**Motivation**: Our team consists of five international graduate students, in the future we will be applying for H1B visa. The visa application process seems very long, complicated and uncertain. So we decided to understand this process and use Machine learning algorithms to predict the acceptance rate and trends of H1B visa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w978Te7MRoFx",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lmhgvIddSRf0",
        "colab": {}
      },
      "source": [
        "!pip install autocorrect\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk,warnings\n",
        "import cleaning as cw\n",
        "import baseline as blc\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "from statistics import mean\n",
        "from autocorrect import Speller \n",
        "from sklearn import metrics\n",
        "from sklearn import model_selection\n",
        "from sklearn.preprocessing import OneHotEncoder #ONE HOT ENCODING\n",
        "from sklearn.ensemble import RandomForestClassifier #Build model - Random Forest Classifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gensim.downloader as api\n",
        "nltk.download('wordnet')\n",
        "nltk.download('words')\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ap8NQkonSOJq"
      },
      "source": [
        "### Data \n",
        "The data used in the project has been collected from <a href=\"https://www.foreignlaborcert.doleta.gov/performancedata.cfm\">the Office of Foreign Labor Certification (OFLC).</a>The Data provides insight into each petition with information such as the Job title, Wage, Employer, Worksite location etc. To get the dataset click on the above link-> click on Disclosure data -> scroll down to H1B data.\\\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3ztV8S9xS40A",
        "colab": {}
      },
      "source": [
        "file2015=pd.read_csv('/content/gdrive/My Drive/H1B_project/H-1B_Disclosure_Data_FY15_Q4.csv',encoding='latin-1', low_memory=False)\n",
        "file2015.rename(columns = {'H-1B_DEPENDENT':'H1B_DEPENDENT','SOC_NAME':'SOC_TITLE','WAGE_RATE_OF_PAY':'WAGE_RATE_OF_PAY_FROM'}, inplace = True)\n",
        "df2015=file2015[['CASE_NUMBER', 'CASE_STATUS', 'CASE_SUBMITTED', 'DECISION_DATE',\n",
        "       'VISA_CLASS','FULL_TIME_POSITION','JOB_TITLE', 'SOC_CODE', 'SOC_TITLE','EMPLOYER_NAME','WAGE_RATE_OF_PAY_FROM',\n",
        "       'WAGE_UNIT_OF_PAY','WORKSITE_CITY','WORKSITE_STATE','H1B_DEPENDENT']]\n",
        "df2015['WAGE_RATE_OF_PAY_FROM']=df2015['WAGE_RATE_OF_PAY_FROM'].str.split('-').str[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Nc0KI4GHTDTF",
        "colab": {}
      },
      "source": [
        "file2016=pd.read_csv('/content/gdrive/My Drive/H1B_project/H-1B_Disclosure_Data_FY16.csv',encoding='latin-1', low_memory=False)\n",
        "file2016.rename(columns = {'H-1B_DEPENDENT':'H1B_DEPENDENT','SOC_NAME':'SOC_TITLE'}, inplace = True)\n",
        "df2016=file2016[['CASE_NUMBER', 'CASE_STATUS', 'CASE_SUBMITTED', 'DECISION_DATE',\n",
        "       'VISA_CLASS','FULL_TIME_POSITION','JOB_TITLE', 'SOC_CODE', 'SOC_TITLE','EMPLOYER_NAME','WAGE_RATE_OF_PAY_FROM',\n",
        "       'WAGE_UNIT_OF_PAY','WORKSITE_CITY','WORKSITE_STATE','H1B_DEPENDENT']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B1r1pOSrTHgc",
        "colab": {}
      },
      "source": [
        "file2017=pd.read_csv('/content/gdrive/My Drive/H1B_project/H-1B_Disclosure_Data_FY17.csv',encoding='latin-1', low_memory=False)\n",
        "file2017.rename(columns = {'SOC_NAME':'SOC_TITLE'}, inplace = True)\n",
        "df2017=file2017[['CASE_NUMBER', 'CASE_STATUS', 'CASE_SUBMITTED', 'DECISION_DATE',\n",
        "       'VISA_CLASS','FULL_TIME_POSITION','JOB_TITLE', 'SOC_CODE', 'SOC_TITLE','EMPLOYER_NAME','WAGE_RATE_OF_PAY_FROM',\n",
        "       'WAGE_UNIT_OF_PAY','WORKSITE_CITY','WORKSITE_STATE','H1B_DEPENDENT']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kj0d8XrfTPd1",
        "colab": {}
      },
      "source": [
        "file2018=pd.read_csv('/content/gdrive/My Drive/H1B_project/H-1B_Disclosure_Data_FY2018_EOY.csv',encoding='latin-1', low_memory=False)\n",
        "file2018.rename(columns = {'SOC_NAME':'SOC_TITLE'}, inplace = True)\n",
        "df2018=file2018[['CASE_NUMBER', 'CASE_STATUS', 'CASE_SUBMITTED', 'DECISION_DATE',\n",
        "       'VISA_CLASS','FULL_TIME_POSITION','JOB_TITLE', 'SOC_CODE', 'SOC_TITLE','EMPLOYER_NAME','WAGE_RATE_OF_PAY_FROM',\n",
        "       'WAGE_UNIT_OF_PAY','WORKSITE_CITY','WORKSITE_STATE','H1B_DEPENDENT']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C6GtHpVRTWet",
        "colab": {}
      },
      "source": [
        "file2019=pd.read_csv('/content/gdrive/My Drive/H1B_project/H-1B_Disclosure_Data_FY2019.csv',encoding='latin-1', low_memory=False)\n",
        "file2019.rename(columns = {'H-1B_DEPENDENT':'H1B_DEPENDENT','WAGE_RATE_OF_PAY_FROM_1':'WAGE_RATE_OF_PAY_FROM','WAGE_UNIT_OF_PAY_1':'WAGE_UNIT_OF_PAY',\\\n",
        "                           'WORKSITE_CITY_1':'WORKSITE_CITY','WORKSITE_STATE_1':'WORKSITE_STATE'}, inplace = True)\n",
        "df2019=file2019[['CASE_NUMBER', 'CASE_STATUS', 'CASE_SUBMITTED', 'DECISION_DATE',\n",
        "       'VISA_CLASS','FULL_TIME_POSITION','JOB_TITLE', 'SOC_CODE', 'SOC_TITLE','EMPLOYER_NAME','WAGE_RATE_OF_PAY_FROM',\n",
        "       'WAGE_UNIT_OF_PAY','WORKSITE_CITY','WORKSITE_STATE','H1B_DEPENDENT']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qCok1RY0VFeM",
        "colab": {}
      },
      "source": [
        "cleaned=pd.concat([df2015,df2016,df2017,df2018,df2019])\n",
        "original=pd.concat([df2015,df2016,df2017,df2018,df2019])\n",
        "cleaned.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QngD94-VEmRu",
        "colab": {}
      },
      "source": [
        "del file2015 # To avoid using the RAM completely\n",
        "del file2016\n",
        "del file2017\n",
        "del file2018\n",
        "del file2019\n",
        "del df2019\n",
        "del df2018\n",
        "del df2017\n",
        "del df2016\n",
        "del df2015"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S2RKhcbJycxS"
      },
      "source": [
        "### Exploratory Data Analysis\n",
        "Before we begin working on our data we need to understand the traits of our data which we accomplish using EDA. We see that we have about 260 columns , not all 260 columns have essential information that contributes to our analysis. Hence we pick out the columns such as case status( Accepted/ Denied) ,Employer, Job title etc.We can look at all the colums and the types of object in each column using cleaned.info(). We also use cleaned['column_name'].value_counts() to find the classes in the column along with the count. For feature engineering we are converting quantitative data to categorical.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jEaANag2w5Uk"
      },
      "source": [
        "####Granularity:\n",
        "Each row in the dataframe represents an application that was filled by the employer and some values are added during the processing of the application such as DECISION_DATE, CASE_STATUS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YI8muywco9DA",
        "colab": {}
      },
      "source": [
        "cleaned.dropna(inplace=True)\n",
        "cleaned.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oG0hPOcJI78r",
        "colab": {}
      },
      "source": [
        "cleaned.head(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y5Pbv_3_mkH_",
        "colab": {}
      },
      "source": [
        "cleaned['VISA_CLASS'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k37klRspm4BW",
        "colab": {}
      },
      "source": [
        "# Visa class has many categories which are not of use , we require only H1B visa type , hence we drop all records with other visa types\n",
        "cleaned.drop(labels=cleaned.loc[cleaned['VISA_CLASS']!='H-1B'].index , inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "efuJruAvnNMV",
        "colab": {}
      },
      "source": [
        "cleaned['CASE_STATUS'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4GXmkkTrnBaI",
        "colab": {}
      },
      "source": [
        "#In case status column we can drop withdrawn records and certified-withdrawn records\n",
        "cleaned.drop(labels=cleaned.loc[cleaned['CASE_STATUS']=='WITHDRAWN'].index , inplace=True)\n",
        "cleaned.drop(labels=cleaned.loc[cleaned['CASE_STATUS']=='CERTIFIED-WITHDRAWN'].index , inplace=True)\n",
        "cleaned['CASE_STATUS'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3aOpaG2Dohr7",
        "colab": {}
      },
      "source": [
        "cleaned.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ebgMvKVMor6Z",
        "colab": {}
      },
      "source": [
        "cleaned['WAGE_RATE_OF_PAY_FROM'].apply(type).value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MJ-yRmY0ehSJ"
      },
      "source": [
        "####Temporality\n",
        "The data is available from 9/9/2019 to 1/1/2015"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M_EkmaiQegGn",
        "colab": {}
      },
      "source": [
        "max_date=cleaned.CASE_SUBMITTED.max()\t\n",
        "min_date=cleaned.CASE_SUBMITTED.min()\n",
        "print(\"The date in the dataset ranges from \"+str(max_date)+\" to\",min_date)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nbfyEEmngLY2",
        "colab": {}
      },
      "source": [
        "#the column wages has a mix of both string and float value types and some record have the symbol '$' which we want to remove\n",
        "cleaned['WAGES']=cleaned['WAGE_RATE_OF_PAY_FROM'].apply(cw.clean_wages).astype('float')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VDi3g4fIgRDm",
        "colab": {}
      },
      "source": [
        "# the wage information that we have available has different unit of pay\n",
        "cleaned['WAGE_UNIT_OF_PAY'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VBCLazyqgXxu",
        "colab": {}
      },
      "source": [
        "# we convert the different units of pay to the type 'Year'\n",
        "cleaned= cw.clean_wageUnit(np,cleaned)\n",
        "cleaned.drop(columns=['WAGE_RATE_OF_PAY_FROM','WAGE_UNIT_OF_PAY'],axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vhMx0hpTjlRR",
        "colab": {}
      },
      "source": [
        "#checking if the WAGES column have outliers\n",
        "print(cleaned['WAGES'].describe())\n",
        "sns.boxplot(y=cleaned['WAGES'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UEGwyt3ukJvG"
      },
      "source": [
        "We can see that we have large outliers , we can remove them from the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2uWva-1ulVgx",
        "colab": {}
      },
      "source": [
        "# Remove the outliers.\n",
        "lowerBound = 0.001\n",
        "upperBound = 0.95\n",
        "result = cleaned.WAGES.quantile([lowerBound, upperBound])\n",
        "cleaned = cleaned[(result.loc[lowerBound] < cleaned.WAGES.values) &\\\n",
        "                  (cleaned.WAGES.values < result.loc[upperBound])]\n",
        "\n",
        "print(cleaned['WAGES'].describe())\n",
        "sns.boxplot(y=cleaned['WAGES'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vJ-OdEdRp_TU",
        "colab": {}
      },
      "source": [
        "#Plot for top 10 companies that have the most applications for H1B visa\n",
        "Top_Employer=cleaned['EMPLOYER_NAME'].value_counts()[:10]\n",
        "plt.figure(figsize=[8,8])\n",
        "ax=sns.barplot(y=Top_Employer.index,x=Top_Employer.values,palette=sns.color_palette('viridis',10))\n",
        "ax.tick_params(labelsize=12)\n",
        "for i, v in enumerate(Top_Employer.values): \n",
        "    ax.text(.5, i, v,fontsize=15,color='white',weight='bold')\n",
        "plt.title('Top 10 Companies sponsoring H1B Visa in 2015-2019', fontsize=20)\n",
        "plt.xlabel(\"Number of applications filed by the companies\",fontsize=15)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XW2MrCJ9rkx5",
        "colab": {}
      },
      "source": [
        "cleaned.WORKSITE_STATE.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "30UMuuYNrzJT",
        "colab": {}
      },
      "source": [
        "cleaned= cw.clean_states(cleaned)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6y8zxWPftW6U"
      },
      "source": [
        "#### Data Scope\n",
        "The data available contains applications submitted by Employers from 2015 till 2019. \\\n",
        "Geographical Scope: The WORKSITE_STATE gives information about the state where the primary worksite location. The data set covers all the states of US. \n",
        "The Data set also cover different Industries majority of the applications are for IT industry but not limited to IT, it also covers Medical, Pharmacy, Law, Carpentery etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KcVj-dd2vjUC",
        "colab": {}
      },
      "source": [
        "cleaned.SOC_TITLE.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XljvOXxQscAQ",
        "colab": {}
      },
      "source": [
        "cleaned.WORKSITE_STATE.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bwN87sFixRWt"
      },
      "source": [
        "####Faithfulness:\n",
        "Each record in the dataframe represents an application filled by the employer, hence we see a lot of spelling mistakes and some punctutaions like ,(comma) and some numbers in the JOB_TITLE, SOC_TITLE feilds, but there are no inconsistencies and data falsification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jlzTPhST7G0B",
        "colab": {}
      },
      "source": [
        "#The Job_title and Soc_title has inconsistencies such as numeric characters and symbols so it has to be cleaned\n",
        "cleaned['JOB_TITLE']=cleaned.JOB_TITLE.apply(lambda txt: \" \".join([cw.remove_num(i) for i in txt.lower().split()]))\n",
        "cleaned['JOB_TITLE']=cleaned['JOB_TITLE'].str.replace(',', '')\n",
        "\n",
        "cleaned['SOC_TITLE']=cleaned.SOC_TITLE.apply(lambda txt: \" \".join([cw.remove_num(i) for i in txt.lower().split()]))\n",
        "cleaned['SOC_TITLE']=cleaned['SOC_TITLE'].str.replace(',', '')\n",
        "cleaned['JOB_TITLE'].value_counts()\n",
        "warnings.simplefilter(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jHWSBSgF7pXX",
        "colab": {}
      },
      "source": [
        "#Function to clean data columns, to correct spelling mistakes and convert plural words to singular\n",
        "cleaned=cw.text_clean(cleaned)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ssEwlv1n7xo1"
      },
      "source": [
        "The has lot of different subcategories which have less than 15 applications in them which does not show much significance , removing them help us condense the data and take only the significant rows.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GoOYWV-bCabm",
        "colab": {}
      },
      "source": [
        "cleaned= cw.drop_less_significant(cleaned)\n",
        "cleaned=cw.cat_to_num(cleaned)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IThvOHguobsP",
        "colab": {}
      },
      "source": [
        "cleaned.loc[(cleaned.CASE_STATUS == \"CERTIFIED\"),\"CASE_STATUS\"] = 1\n",
        "cleaned.loc[(cleaned.CASE_STATUS == \"DENIED\"),\"CASE_STATUS\"] = 0\n",
        "cleaned.loc[(cleaned.FULL_TIME_POSITION == 'Y'),\"FULL_TIME_POSITION\"] = 1\n",
        "cleaned.loc[(cleaned.FULL_TIME_POSITION == 'N'),\"FULL_TIME_POSITION\"] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uPEVdBTo1ZC3",
        "colab": {}
      },
      "source": [
        "data_scnt,data_anlst,data_eng,mach_learn=cw.data_jobs(cleaned)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5eM8UzR-17Og"
      },
      "source": [
        "## Are you a international student? Are you looking for jobs in Data Science domain?\n",
        "The below visualization shows median salary for four data science related jobs and the number of visa applications that are submitted for each job. This gives us insights about the jobs available for international students/workers about the job market for Data Science domain. We get to know the number of jobs available for each caegory as well as the median wage for each category. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QH1xs65O2Dp3",
        "colab": {}
      },
      "source": [
        "f,ax=plt.subplots(figsize=(8,6))\n",
        "bplot1=plt.boxplot([data_scnt[data_scnt['WAGES']<200000].WAGES,data_anlst[data_anlst['WAGES']<200000].WAGES,data_eng[data_eng['WAGES']<200000].WAGES,mach_learn[mach_learn['WAGES']<200000].WAGES],patch_artist=\"True\")\n",
        "ax.set_xticklabels(['Data Scientists','Data Analysts','Data Engineer','Machine Learning'],fontsize=15)\n",
        "ax.set_title('Salary Distribution for jobs in Data Science field in 2019', fontsize=15)\n",
        "ax.tick_params(labelsize=10)\n",
        "colors = ['blue','orange', 'green', 'red'] \n",
        "for patch, color in zip(bplot1['boxes'], colors): \n",
        "  patch.set_facecolor(color)\n",
        "plt.show()\n",
        "datajobs=cw.data_concat(pd,data_scnt,data_anlst,data_eng,mach_learn)\n",
        "ax2=sns.countplot(x=\"data\", data=datajobs)\n",
        "ax2.set_title('Number of petitions for each jobs in Data Science field in 2019', fontsize=15)\n",
        "warnings.simplefilter(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7H3wXPnEnse3"
      },
      "source": [
        "**Features considered for the classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mNftZfY8lCCE",
        "colab": {}
      },
      "source": [
        "Dataset = cleaned[[\"CASE_STATUS\", \"FULL_TIME_POSITION\",\"SOC_TITLE\", \"WORKSITE_STATE\", \"WAGES\", \"H1B_DEPENDENT\"]]\n",
        "Dataset.reset_index(drop = True,inplace = True)\n",
        "Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fJqE8futomje"
      },
      "source": [
        "**CONVERTING CATEGORICAL COLUMNS TO NUMERIC COLUMNS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r5QiHud8lUTI",
        "colab": {}
      },
      "source": [
        "# Cleaning SOC_TITLE(job_positions) column by removing punctuations\n",
        "Dataset=cw.fun_punctuation(Dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fg6RHFPqpD4k"
      },
      "source": [
        "**If we have more number of unique values in the categorical columns we can use word_vectors to convert them into numeric columns as one-hot encoding fails to convert.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pzg1qFcIl2Jh",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(columns = [\"job_position\"])\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GnHViS-BmAgl",
        "colab": {}
      },
      "source": [
        "# converting job titles to numeric values using word_vectors.\n",
        "Dataset = cw.fun_word_vectors(Dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfGsN7dBtCA1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[\"job_position\"] = Dataset[\"SOC_TITLE\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qfGdfLUTmDBK",
        "colab": {}
      },
      "source": [
        "Dataset.drop(columns=[\"SOC_TITLE\"],axis=1,inplace=True)\n",
        "Dataset.reset_index(drop = True, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tdPXmezNmKZM",
        "colab": {}
      },
      "source": [
        "# one hot encoding to convert states column to numeric values using one hot encoding\n",
        "Encoding = OneHotEncoder(handle_unknown='ignore',sparse = True)\n",
        "Encoding_df = pd.DataFrame(Encoding.fit_transform(Dataset[[\"WORKSITE_STATE\"]]).toarray())\n",
        "Dataset = Dataset.join(Encoding_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qkZK5mdxma5U",
        "colab": {}
      },
      "source": [
        "Dataset.drop(columns=['WORKSITE_STATE'],axis=1,inplace=True)\n",
        "word_vectors = df[[\"job_position\"]].values\n",
        "word_vectors.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O4THhCcLm1-V",
        "colab": {}
      },
      "source": [
        "temp = np.vstack(word_vectors.tolist())\n",
        "temp.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xk1xerqrquLB",
        "colab": {}
      },
      "source": [
        "# Y represents the output labels\n",
        "Y = Dataset[\"CASE_STATUS\"].astype(float)\n",
        "Dataset.drop(columns = [\"CASE_STATUS\"],inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xGHyM_iTnEiD",
        "colab": {}
      },
      "source": [
        "# all_features represent the input labels\n",
        "other_features = Dataset.iloc[:].values\n",
        "all_features = np.hstack([temp, other_features])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tSqJobhYOj-0",
        "colab": {}
      },
      "source": [
        "#Dependent and independent variables\n",
        "X = all_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "52vYSJW8FjF5"
      },
      "source": [
        "\n",
        "### Baseline classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-lLzCf96GZiD"
      },
      "source": [
        "The baseline classifier is done with a basic model. In this case we are taking the mean of the labels ('certified' and 'denied' for H1B visa approvals). It will give us the base accuracy to which we will compare our classifier's accuracy. Our classifier should have a better accuracy than the baseline classifier accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cCDCRjAqF1TB",
        "colab": {}
      },
      "source": [
        "# This step assigns a binary class label (0 or 1) to each label for H1B visa approval. \n",
        "import importlib\n",
        "importlib.reload(blc)\n",
        "y = Y.to_numpy().astype(int) # Groundtruth labels for the dataset\n",
        "counts = cleaned['CASE_STATUS'].value_counts()\n",
        "print(counts)\n",
        "print('proportion: ', counts[1]/counts[0], ': 1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EBm_dFDZF8LD",
        "colab": {}
      },
      "source": [
        "accuracies = []\n",
        "AUCs = []\n",
        "kf = sklearn.model_selection.KFold(n_splits=4, random_state=1, shuffle=True)# Testing with K-folds \n",
        "for train_idx, test_idx in kf.split(X):\n",
        "    X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx] \n",
        "    prediction = blc.run_clasifier(X_train, y_train, X_test, np)\n",
        "    fold_accuracy = blc.compute_accuracy(y_test, prediction)\n",
        "    fold_AUC = blc.compute_AUC(y_test, prediction)\n",
        "    accuracies.append(fold_accuracy)\n",
        "    if fold_AUC != None: AUCs.append(fold_AUC)\n",
        "baseline_clasifier_accuracy = mean(accuracies)\n",
        "print('Baseline accuracy (K-fold): ', baseline_clasifier_accuracy)\n",
        "print('AUC K-fold: ', mean(AUCs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O1BXDP3TGD8-"
      },
      "source": [
        "The initial accuracy results of the baseline classifier was 0.99. This result is due to the highly imbalanced data with respect to the Case Status, where there are 624682 CERTIFIED applications and 5158 DENIED applications. The proportion of it is 134.02 to 1. Therefore, a performance measure based on the accuracy is not a good one. A better performance measure in imbalanced data is the Area under the ROC Curve (AUC). It meassures the likelihood that given two random points (one from the positive and one from the negative class) the classifier will rank the point from the positive class higher than the one from the negative one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YRiXS6aKyKz3"
      },
      "source": [
        "### Balancing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M8Mq8K89lpqz"
      },
      "source": [
        "Given the inbalanced data we have, we lack observations required to train an effective model that classifies the status of the H1B visa. To addess that, a good idea is to upsample the minority class and undersample the majority class. First, the ADASYN algorithm is applied to upsample the minority class. It generates new related samples and some random samples to avoid linear correlation to the parent. Second, the ENN algorithm is applied to undersample the majority class. It focuses on removing the noisy observations. The expected proportion between the two classes is 1 to 2 at most."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qybksxpSqxFn",
        "colab": {}
      },
      "source": [
        "# Upsampling the minority class\n",
        "from imblearn.over_sampling import ADASYN\n",
        "ada = ADASYN(sampling_strategy=0.5, random_state=0)\n",
        "X_ada, y_ada = ada.fit_resample(X, Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-OOFug6uq04x",
        "colab": {}
      },
      "source": [
        "# Downsampling the majority class\n",
        "from imblearn.under_sampling import EditedNearestNeighbours\n",
        "enn = EditedNearestNeighbours()\n",
        "X, Y = enn.fit_resample(X_ada, y_ada)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sweKaPRZq5eL",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "count_y_ada_enn = Counter(Y)\n",
        "print('Balanced dataset shape %s' % count_y_ada_enn)\n",
        "print('proportion: ', count_y_ada_enn[1]/count_y_ada_enn[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YpX2h3fhF_Du",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25)# Testing with regular split\n",
        "prediction_bl = blc.run_clasifier(X_train, y_train, X_test, np)\n",
        "split_accuracy = blc.compute_accuracy(y_test, prediction_bl)\n",
        "split_AUC = blc.compute_AUC(y_test, prediction_bl)\n",
        "print('Baseline accuracy (split): ', split_accuracy)  \n",
        "print('AUC split (80-20): ', split_AUC)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G_ZttjowsM6M",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "from matplotlib import pyplot\n",
        "def plot_precision_recall (y_test, probs):\n",
        "    pos_probs = probs[:, 1] if probs.ndim > 1 else probs\n",
        "    pyplot.figure(figsize=(12,8))\n",
        "    # calculate model precision-recall curve\n",
        "    precision, recall, _ = precision_recall_curve(y_test, pos_probs)\n",
        "    print('Precision and Recall: ', precision, recall)\n",
        "    # plot the model precision-recall curve\n",
        "    pyplot.plot(recall, precision, marker='.', label='Logistic')\n",
        "    # axis labels\n",
        "    pyplot.xlabel('Recall')\n",
        "    pyplot.ylabel('Precision')\n",
        "    pyplot.legend()\n",
        "    pyplot.title('Precision vs Recall curve', loc='center')\n",
        "    return pyplot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h39Oi-z5rjOZ",
        "colab": {}
      },
      "source": [
        "# Confusion matrix\n",
        "tn, fp, fn, tp = metrics.confusion_matrix(y_test, prediction_bl).ravel()\n",
        "print(\"tn, fp, fn, tp\", (tn, fp, fn, tp))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a0P24Xnlr40h",
        "colab": {}
      },
      "source": [
        "# classification report\n",
        "report = metrics.classification_report(y_test, prediction_bl)\n",
        "print(report)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8-xGF0ehsTaz",
        "colab": {}
      },
      "source": [
        "# Plot precision recall curve\n",
        "plot_precision_recall(y_test, prediction_bl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "59v_tJlThq6d"
      },
      "source": [
        "### Random forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zrMgSvnoPMN6",
        "colab": {}
      },
      "source": [
        "#Split the date into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3IXHXqRSPUfB",
        "colab": {}
      },
      "source": [
        "#Build model - Random Forest Classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#model_clf = RandomForestClassifier(n_estimators=10, random_state=30)\n",
        "model_clf = RandomForestClassifier(n_jobs=2,random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tzWotUaEPX9p",
        "colab": {}
      },
      "source": [
        "#train the model\n",
        "model_clf.fit(X_train,y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4GOkp9nvPbkU",
        "colab": {}
      },
      "source": [
        "#test the model (predict with our test data)\n",
        "prediction_test = model_clf.predict(X_test)\n",
        "prediction_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k9xAQ6fCPict",
        "colab": {}
      },
      "source": [
        "#compare with original value, Y_test\n",
        "from sklearn import metrics\n",
        "print(\"Accuracy = \", metrics.accuracy_score(y_test, prediction_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nsJPocIrrn27",
        "colab": {}
      },
      "source": [
        "# Confusion matrix\n",
        "metrics.confusion_matrix(y_test, prediction_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fqxrtvO_rn29",
        "colab": {}
      },
      "source": [
        "# classification report\n",
        "report = metrics.classification_report(y_test, prediction_test)\n",
        "report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GpbY4QVSsgv5",
        "colab": {}
      },
      "source": [
        "y_probs_rand_forest = model_clf.predict_proba(X_test)\n",
        "plot_precision_recall (y_test, y_probs_rand_forest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z0ZnYIKr98kv"
      },
      "source": [
        "##### **PRESIDENT TRUMP's \"Buy American and Hire American\" EXECUTIVE ORDER's IMPACT on H1B visa**\n",
        "The \"Buy American and Hire American\" order was passed on April 18, 2017, to Order to protect American workers and strengthen the American economy. Among other purposes, it aims to detect and prevent fraud in the process. Also, to avoid the abuse of the program which is heavily used by technology companies. This statement is interesting to evaluate from our perspective as international students.\n",
        "The after-effects of which caused a gradual decrease in the request and acceptance of H1B visas have been creating more job opportunities for immigrants which in turn contributes to the American workforce.\n",
        "\n",
        "It was also observed that just before this order was passed there was a surge in the applications submitted. \n",
        "\n",
        "As the graph demonstrates, this type of order, after it has been approved, take some months (around 6 months in this case)  to show the after-effects. It's in 2018 when we see the number of applications and their acceptance is falling so far, as shown in the below visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q4W71q9s9xgb",
        "colab": {}
      },
      "source": [
        "order_date = '18-04-2017'\n",
        "srt='01-01-2015'\n",
        "\n",
        "original['CASE_SUBMITTED']=pd.to_datetime(original['CASE_SUBMITTED'])\n",
        "mask = (original['CASE_SUBMITTED'] < order_date )& (original['CASE_SUBMITTED'] >= srt)\n",
        "mask1 = (original['CASE_SUBMITTED'] >= order_date)\n",
        "cleaned_before = original.loc[mask]\n",
        "cleaned_after= original.loc[mask1]\n",
        "cleaned_before =cleaned_before.sample(n=10000, replace=False, random_state=1)\n",
        "cleaned_after =cleaned_after.sample(n=10000, replace=False, random_state=1)\n",
        "policy_dataframe=pd.concat([cleaned_before,cleaned_after]).reset_index(drop=True)\n",
        "policy_dataframe = policy_dataframe[['CASE_STATUS','CASE_SUBMITTED','CASE_NUMBER']]\n",
        "policy_dataframe=policy_dataframe.replace({'CERTIFIED-WITHDRAWN': 'CERTIFIED'})\n",
        "policy_dataframe['CASE_SUBMITTED']= policy_dataframe['CASE_SUBMITTED'].dt.year\n",
        "\n",
        "policy_dataframe.drop(labels=policy_dataframe.loc[policy_dataframe['CASE_STATUS']=='WITHDRAWN'].index, inplace = True)\n",
        "policy_dataframe.drop(labels=policy_dataframe.loc[policy_dataframe['CASE_STATUS']=='DENIED'].index, inplace = True)\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "line_certified = pd.crosstab(policy_dataframe.CASE_SUBMITTED,policy_dataframe.CASE_STATUS).plot()#title='H1B Visas Certified by year'\n",
        "plt.title('H1B Visas Certified by year', fontsize=22)\n",
        "plt.legend(title='# Visas')\n",
        "plt.axvline(x= 2017.5, color = 'r')\n",
        "plt.xlabel(\"Year\")\n",
        "plt.annotate(\n",
        "        'Hire American Executive Order',\n",
        "        fontsize=14,\n",
        "        color='r',\n",
        "        xy=(2017.5, 4150), #400 \n",
        "        #arrowprops=dict(arrowstyle='->'),\n",
        "        arrowprops=dict(arrowstyle='->',connectionstyle='arc3,rad=.2'),\n",
        "        xytext=(2015.5, 4150)) #405\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IXwKy7rB-i0V"
      },
      "source": [
        "From the above visualization made on a random sample of about 20000 records from 2015 to 2019, we can infer that the visa acceptance is gradually seen as a downfall, which is one the after-effects of the executive order passed earlier 2017.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zt7yGkSTXiaP",
        "colab": {}
      },
      "source": [
        "#deleting the temporary dataframe created to avoid ram consumption.\n",
        "del cleaned_before\n",
        "del cleaned_after\n",
        "del original\n",
        "del policy_dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eIo6zyaJsWly",
        "colab": {}
      },
      "source": [
        "#mapping wages to 4 categories\n",
        "cleaned['WAGE_CATEGORY'] =pd.cut(cleaned.WAGES,bins=[0,50000,100000,140000,200000],\n",
        "                                 labels=['LOW','AVERAGE','HIGH',\"VERY HIGH\"])\n",
        "df_temp = pd.DataFrame(columns = [\"WC_NUM\"])\n",
        "cleaned['WC_NUM'] = pd.cut(cleaned.WAGES,bins=[0,50000,100000,140000,200000],labels=[0,1,2,3])\n",
        "df_temp[\"WC_NUM\"]=pd.cut(cleaned.WAGES,bins=[0,50000,100000,140000,200000],labels=[0,1,2,3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ra-Hst33s87V",
        "colab": {}
      },
      "source": [
        "#assigning X, y for baseline classifier.\n",
        "X = cleaned['WC_NUM'].to_numpy()\n",
        "y = df_temp[\"WC_NUM\"].to_numpy().astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN4oS2HT6USi",
        "colab_type": "text"
      },
      "source": [
        "### **WAGE RANGE RANGE PREDICTION:**\n",
        "Using the data from our dataset we divide the wages into 4 categories which is LOW, AVERAGE, HIGH, VERY_HIGH.\n",
        "And this predictor will predict the expected salary/wage range for any H1B visa holder or an H1B aspirant based on various factors like location , occupation ,employer etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF792HmH6UmO",
        "colab_type": "text"
      },
      "source": [
        "**BASELINE CLASSIFIER FOR WAGE RANGE PREDICTOR:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XTz__p7RtNe_",
        "colab": {}
      },
      "source": [
        "accuracies = []\n",
        "AUCs = []\n",
        "kf = sklearn.model_selection.KFold(n_splits=4, random_state=1, shuffle=True)# Testing with K-folds \n",
        "for train_idx, test_idx in kf.split(X):\n",
        "    X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx] \n",
        "    prediction = blc.run_clasifier_(X_train, y_train, X_test, np)\n",
        "    fold_accuracy = blc.compute_accuracy(y_test, prediction)\n",
        "    fold_AUC = blc.compute_AUC(y_test, prediction)\n",
        "    accuracies.append(fold_accuracy)\n",
        "    if fold_AUC != None: AUCs.append(fold_AUC)\n",
        "baseline_clasifier_accuracy = mean(accuracies)\n",
        "print('Baseline accuracy (K-fold): ', baseline_clasifier_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7JK1GcwntT1Q",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)# Testing with regular split\n",
        "prediction = blc.run_clasifier_(X_train, y_train, X_test, np)\n",
        "split_accuracy = blc.compute_accuracy(y_test, prediction)\n",
        "#split_AUC = multiclass_roc_auc_score(y_test, prediction)\n",
        "print('Baseline accuracy (split): ', split_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BxQHQdYEtbOV"
      },
      "source": [
        "**Features considered for the wage classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4XsLFYb0tlWG",
        "colab": {}
      },
      "source": [
        "order_date = '01-01-2018'\n",
        "cleaned['CASE_SUBMITTED']=pd.to_datetime(cleaned['CASE_SUBMITTED'])\n",
        "mask1 = (cleaned['CASE_SUBMITTED'] < order_date)\n",
        "cleaned_before = cleaned.loc[mask1]\n",
        "Dataset_wage=cleaned_before[[\"EMPLOYER_NAME\",\"FULL_TIME_POSITION\",\"SOC_TITLE\", \"WORKSITE_STATE\", \"WC_NUM\",\"CASE_STATUS\"]]\n",
        "Dataset_wage.reset_index(drop = True,inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4LAAiyi3rn3A",
        "colab": {}
      },
      "source": [
        "# Considering top 70 employers and remaining employers as others\n",
        "Top_EMPLOYER = Dataset_wage[\"EMPLOYER_NAME\"].value_counts().head(70)\n",
        "Top_EMPLOYER\n",
        "def emp_function(emp):\n",
        "    if emp in Top_EMPLOYER:\n",
        "        return emp\n",
        "    else:\n",
        "        return \"others\"\n",
        "Dataset_wage[\"TOP_EMPLOYER\"] = Dataset_wage[\"EMPLOYER_NAME\"].apply(emp_function)\n",
        "Dataset_wage[\"TOP_EMPLOYER\"].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B0hEAngPtqz7",
        "colab": {}
      },
      "source": [
        "Dataset_wage.drop(columns=[\"EMPLOYER_NAME\"],axis=1,inplace=True)\n",
        "print(Dataset_wage.info())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O1FufvJDtujp",
        "colab": {}
      },
      "source": [
        "df_wage = pd.DataFrame(columns = [\"soc_title\"])\n",
        "Dataset_wage[\"SOC_TITLE\"] = Dataset_wage[\"SOC_TITLE\"].apply(lambda x : remove_punctuation(x))\n",
        "df_wage[\"soc_title\"] = Dataset_wage[\"SOC_TITLE\"].apply(lambda text : grouping(text)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j5EY_vJetvd6",
        "colab": {}
      },
      "source": [
        "Dataset_wage.drop(columns=[\"SOC_TITLE\"],axis=1,inplace=True)\n",
        "Dataset_wage.reset_index(drop = True, inplace = True)\n",
        "#print(Dataset_wage.info())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "56YzptYyt5Q1",
        "colab": {}
      },
      "source": [
        "Encoding = OneHotEncoder(handle_unknown='ignore',sparse = True)\n",
        "Encoding_wage = pd.DataFrame(Encoding.fit_transform(Dataset_wage[[\"WORKSITE_STATE\",\"TOP_EMPLOYER\"]]).toarray())\n",
        "Dataset_wage = Dataset_wage.join(Encoding_wage)\n",
        "Dataset_wage.drop(columns=[\"WORKSITE_STATE\",\"TOP_EMPLOYER\"],axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tb3jtL7Ht8Vu",
        "colab": {}
      },
      "source": [
        "Dataset_wage.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WoiCFeD5uAiC",
        "colab": {}
      },
      "source": [
        "wv_wage = df_wage[[\"soc_title\"]].values\n",
        "temp_var = np.vstack(wv_wage.tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8lTfP36duC36",
        "colab": {}
      },
      "source": [
        "Y = Dataset_wage[\"WC_NUM\"].astype(float)\n",
        "Dataset_wage.drop(columns = [\"WC_NUM\"],inplace = True)\n",
        "# sum_features represent the input data \n",
        "other_features = Dataset_wage.iloc[:].values\n",
        "del Dataset_wage\n",
        "other_features.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZFgOr68HuIxP",
        "colab": {}
      },
      "source": [
        "sum_features = np.hstack([temp_var, other_features])\n",
        "X = sum_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wHaDpWvwu_hY",
        "colab": {}
      },
      "source": [
        "#Split the date into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X9sthvgPuMji",
        "colab": {}
      },
      "source": [
        "dec_clf = DecisionTreeClassifier(max_depth=15,criterion='gini')\n",
        "dec_clf.fit(X_train,y_train)#train the model\n",
        "prediction_test = dec_clf.predict(X_test)#test the model (predict with our test data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GeprScgguXFE",
        "colab": {}
      },
      "source": [
        "print(\"Accuracy = \", metrics.accuracy_score(y_test, prediction_test))#compare with original value, Y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5ZGgDm8buaNQ",
        "colab": {}
      },
      "source": [
        "metrics.confusion_matrix(y_test, prediction_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VRXhig74ugPZ",
        "colab": {}
      },
      "source": [
        "report = metrics.classification_report(y_test, prediction_test)\n",
        "print(report)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}