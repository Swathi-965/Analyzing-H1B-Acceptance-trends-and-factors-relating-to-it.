{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "H1B_Visa_Data_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-S6Q8i5yQ_zF"
      },
      "source": [
        "## Analysing H1B Visa data Trends"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G3DkkMAuRQ9j"
      },
      "source": [
        "H1B visa is a nonimmigrant visa issued to gradaute level applicants allowing them to work in the United States. The employer sponsors the H1B visa for workers with theoretical or technical expertise in specialized fields such as in IT, finance, accounting etc. An interesting fact about immigrant workers is that about 52 percent of new Silicon valley companies were founded by such workers during 1995 and 2005. Some famous CEOs like Indira Nooyi (Pepsico), Elon Musk (Tesla), Sundar Pichai (Google),Satya Nadella (Microsoft) once arrived to the US on a H1B visa.\\\n",
        "**Motivation**: Our team consists of five international graduate students, in the future we will be applying for H1B visa. The visa application process seems very long, complicated and uncertain. So we decided to understand this process and use Machine learning algorithms to predict the acceptance rate and trends of H1B visa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w978Te7MRoFx",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lmhgvIddSRf0",
        "colab": {}
      },
      "source": [
        "!pip install autocorrect\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk,warnings\n",
        "import clean_wage as cw\n",
        "import baseline as blc\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "from statistics import mean\n",
        "from autocorrect import Speller \n",
        "from sklearn import metrics\n",
        "from sklearn import model_selection\n",
        "from sklearn.preprocessing import OneHotEncoder #ONE HOT ENCODING\n",
        "from sklearn.ensemble import RandomForestClassifier #Build model - Random Forest Classifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import gensim.downloader as api\n",
        "nltk.download('wordnet')\n",
        "nltk.download('words')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ap8NQkonSOJq"
      },
      "source": [
        "### Data \n",
        "The data used in the project has been collected from <a href=\"https://www.foreignlaborcert.doleta.gov/performancedata.cfm\">the Office of Foreign Labor Certification (OFLC).</a>The Data provides insight into each petition with information such as the Job title, Wage, Employer, Worksite location etc. To get the dataset click on the above link-> click on Disclosure data -> scroll down to H1B data.\\\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3ztV8S9xS40A",
        "colab": {}
      },
      "source": [
        "file2015=pd.read_csv('/content/gdrive/My Drive/H1B_project/H-1B_Disclosure_Data_FY15_Q4.csv',encoding='latin-1', low_memory=False)\n",
        "file2015.rename(columns = {'H-1B_DEPENDENT':'H1B_DEPENDENT','SOC_NAME':'SOC_TITLE','WAGE_RATE_OF_PAY':'WAGE_RATE_OF_PAY_FROM'}, inplace = True)\n",
        "df2015=file2015[['CASE_NUMBER', 'CASE_STATUS', 'CASE_SUBMITTED', 'DECISION_DATE',\n",
        "       'VISA_CLASS','FULL_TIME_POSITION','JOB_TITLE', 'SOC_CODE', 'SOC_TITLE','EMPLOYER_NAME','WAGE_RATE_OF_PAY_FROM',\n",
        "       'WAGE_UNIT_OF_PAY','WORKSITE_CITY','WORKSITE_STATE','H1B_DEPENDENT']]\n",
        "df2015['WAGE_RATE_OF_PAY_FROM']=df2015['WAGE_RATE_OF_PAY_FROM'].str.split('-').str[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Nc0KI4GHTDTF",
        "colab": {}
      },
      "source": [
        "file2016=pd.read_csv('/content/gdrive/My Drive/H1B_project/H-1B_Disclosure_Data_FY16.csv',encoding='latin-1', low_memory=False)\n",
        "file2016.rename(columns = {'H-1B_DEPENDENT':'H1B_DEPENDENT','SOC_NAME':'SOC_TITLE'}, inplace = True)\n",
        "df2016=file2016[['CASE_NUMBER', 'CASE_STATUS', 'CASE_SUBMITTED', 'DECISION_DATE',\n",
        "       'VISA_CLASS','FULL_TIME_POSITION','JOB_TITLE', 'SOC_CODE', 'SOC_TITLE','EMPLOYER_NAME','WAGE_RATE_OF_PAY_FROM',\n",
        "       'WAGE_UNIT_OF_PAY','WORKSITE_CITY','WORKSITE_STATE','H1B_DEPENDENT']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B1r1pOSrTHgc",
        "colab": {}
      },
      "source": [
        "file2017=pd.read_csv('/content/gdrive/My Drive/H1B_project/H-1B_Disclosure_Data_FY17.csv',encoding='latin-1', low_memory=False)\n",
        "file2017.rename(columns = {'SOC_NAME':'SOC_TITLE'}, inplace = True)\n",
        "df2017=file2017[['CASE_NUMBER', 'CASE_STATUS', 'CASE_SUBMITTED', 'DECISION_DATE',\n",
        "       'VISA_CLASS','FULL_TIME_POSITION','JOB_TITLE', 'SOC_CODE', 'SOC_TITLE','EMPLOYER_NAME','WAGE_RATE_OF_PAY_FROM',\n",
        "       'WAGE_UNIT_OF_PAY','WORKSITE_CITY','WORKSITE_STATE','H1B_DEPENDENT']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kj0d8XrfTPd1",
        "colab": {}
      },
      "source": [
        "file2018=pd.read_csv('/content/gdrive/My Drive/H1B_project/H-1B_Disclosure_Data_FY2018_EOY.csv',encoding='latin-1', low_memory=False)\n",
        "file2018.rename(columns = {'SOC_NAME':'SOC_TITLE'}, inplace = True)\n",
        "df2018=file2018[['CASE_NUMBER', 'CASE_STATUS', 'CASE_SUBMITTED', 'DECISION_DATE',\n",
        "       'VISA_CLASS','FULL_TIME_POSITION','JOB_TITLE', 'SOC_CODE', 'SOC_TITLE','EMPLOYER_NAME','WAGE_RATE_OF_PAY_FROM',\n",
        "       'WAGE_UNIT_OF_PAY','WORKSITE_CITY','WORKSITE_STATE','H1B_DEPENDENT']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C6GtHpVRTWet",
        "colab": {}
      },
      "source": [
        "file2019=pd.read_csv('/content/gdrive/My Drive/H1B_project/H-1B_Disclosure_Data_FY2019.csv',encoding='latin-1', low_memory=False)\n",
        "file2019.rename(columns = {'H-1B_DEPENDENT':'H1B_DEPENDENT','WAGE_RATE_OF_PAY_FROM_1':'WAGE_RATE_OF_PAY_FROM','WAGE_UNIT_OF_PAY_1':'WAGE_UNIT_OF_PAY',\\\n",
        "                           'WORKSITE_CITY_1':'WORKSITE_CITY','WORKSITE_STATE_1':'WORKSITE_STATE'}, inplace = True)\n",
        "df2019=file2019[['CASE_NUMBER', 'CASE_STATUS', 'CASE_SUBMITTED', 'DECISION_DATE',\n",
        "       'VISA_CLASS','FULL_TIME_POSITION','JOB_TITLE', 'SOC_CODE', 'SOC_TITLE','EMPLOYER_NAME','WAGE_RATE_OF_PAY_FROM',\n",
        "       'WAGE_UNIT_OF_PAY','WORKSITE_CITY','WORKSITE_STATE','H1B_DEPENDENT']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qCok1RY0VFeM",
        "colab": {}
      },
      "source": [
        "cleaned=pd.concat([df2015,df2016,df2017,df2018,df2019])\n",
        "cleaned.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QngD94-VEmRu",
        "colab": {}
      },
      "source": [
        "del file2015 # To avoid using the RAM completely\n",
        "del file2016\n",
        "del file2017\n",
        "del file2018\n",
        "del file2019\n",
        "del df2019\n",
        "del df2018\n",
        "del df2017\n",
        "del df2016\n",
        "del df2015"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S2RKhcbJycxS"
      },
      "source": [
        "### Exploratory Data Analysis\n",
        "Before we begin working on our data we need to understand the traits of our data which we accomplish using EDA. We see that we have about 260 columns , not all 260 columns have essential information that contributes to our analysis. Hence we pick out the columns such as case status( Accepted/ Denied) ,Employer, Job title etc.We can look at all the colums and the types of object in each column using cleaned.info(). We also use cleaned['column_name'].value_counts() to find the classes in the column along with the count. For feature engineering we are converting quantitative data to categorical.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jEaANag2w5Uk"
      },
      "source": [
        "####Granularity:\n",
        "Each row in the dataframe represents an application that was filled by the employer and some values are added during the processing of the application such as DECISION_DATE, CASE_STATUS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YI8muywco9DA",
        "colab": {}
      },
      "source": [
        "cleaned.dropna(inplace=True)\n",
        "cleaned.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oG0hPOcJI78r",
        "colab": {}
      },
      "source": [
        "cleaned.head(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y5Pbv_3_mkH_",
        "colab": {}
      },
      "source": [
        "cleaned['VISA_CLASS'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k37klRspm4BW",
        "colab": {}
      },
      "source": [
        "# Visa class has many categories which are not of use , we require only H1B visa type , hence we drop all records with other visa types\n",
        "cleaned.drop(labels=cleaned.loc[cleaned['VISA_CLASS']!='H-1B'].index , inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "efuJruAvnNMV",
        "colab": {}
      },
      "source": [
        "cleaned['CASE_STATUS'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4GXmkkTrnBaI",
        "colab": {}
      },
      "source": [
        "#In case status column we can drop withdrawn records and certified-withdrawn records\n",
        "cleaned.drop(labels=cleaned.loc[cleaned['CASE_STATUS']=='WITHDRAWN'].index , inplace=True)\n",
        "cleaned.drop(labels=cleaned.loc[cleaned['CASE_STATUS']=='CERTIFIED-WITHDRAWN'].index , inplace=True)\n",
        "cleaned['CASE_STATUS'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3aOpaG2Dohr7",
        "colab": {}
      },
      "source": [
        "cleaned.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ebgMvKVMor6Z",
        "colab": {}
      },
      "source": [
        "cleaned['WAGE_RATE_OF_PAY_FROM'].apply(type).value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MJ-yRmY0ehSJ"
      },
      "source": [
        "####Temporality\n",
        "The data is available from 9/9/2019 to 1/1/2015"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M_EkmaiQegGn",
        "colab": {}
      },
      "source": [
        "max_date=cleaned.CASE_SUBMITTED.max()\t\n",
        "min_date=cleaned.CASE_SUBMITTED.min()\n",
        "print(\"The date in the dataset ranges from \"+str(max_date)+\" to\",min_date)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nbfyEEmngLY2",
        "colab": {}
      },
      "source": [
        "#the column wages has a mix of both string and float value types and some record have the symbol '$' which we want to remove\n",
        "cleaned['WAGES']=cleaned['WAGE_RATE_OF_PAY_FROM'].apply(cw.clean_wages).astype('float')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VDi3g4fIgRDm",
        "colab": {}
      },
      "source": [
        "# the wage information that we have available has different unit of pay\n",
        "cleaned['WAGE_UNIT_OF_PAY'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VBCLazyqgXxu",
        "colab": {}
      },
      "source": [
        "# we convert the different units of pay to the type 'Year'\n",
        "cleaned= cw.clean_wageUnit(np,cleaned)\n",
        "cleaned.drop(columns=['WAGE_RATE_OF_PAY_FROM','WAGE_UNIT_OF_PAY'],axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vhMx0hpTjlRR",
        "colab": {}
      },
      "source": [
        "#checking if the WAGES column have outliers\n",
        "print(cleaned['WAGES'].describe())\n",
        "sns.boxplot(y=cleaned['WAGES'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UEGwyt3ukJvG"
      },
      "source": [
        "We can see that we have large outliers , we can remove them from the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2uWva-1ulVgx",
        "colab": {}
      },
      "source": [
        "# Remove the outliers.\n",
        "lowerBound = 0.001\n",
        "upperBound = 0.95\n",
        "result = cleaned.WAGES.quantile([lowerBound, upperBound])\n",
        "cleaned = cleaned[(result.loc[lowerBound] < cleaned.WAGES.values) &\\\n",
        "                  (cleaned.WAGES.values < result.loc[upperBound])]\n",
        "\n",
        "print(cleaned['WAGES'].describe())\n",
        "sns.boxplot(y=cleaned['WAGES'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vJ-OdEdRp_TU",
        "colab": {}
      },
      "source": [
        "#Plot for top 10 companies that have the most applications for H1B visa\n",
        "Top_Employer=cleaned['EMPLOYER_NAME'].value_counts()[:10]\n",
        "plt.figure(figsize=[8,8])\n",
        "ax=sns.barplot(y=Top_Employer.index,x=Top_Employer.values,palette=sns.color_palette('viridis',10))\n",
        "ax.tick_params(labelsize=12)\n",
        "for i, v in enumerate(Top_Employer.values): \n",
        "    ax.text(.5, i, v,fontsize=15,color='white',weight='bold')\n",
        "plt.title('Top 10 Companies sponsoring H1B Visa in 2015-2019', fontsize=20)\n",
        "plt.xlabel(\"Number of applications filed by the companies\",fontsize=15)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XW2MrCJ9rkx5",
        "colab": {}
      },
      "source": [
        "cleaned.WORKSITE_STATE.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "30UMuuYNrzJT",
        "colab": {}
      },
      "source": [
        "cleaned= cw.clean_states(cleaned)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6y8zxWPftW6U"
      },
      "source": [
        "#### Data Scope\n",
        "The data available contains applications submitted by Employers from 2015 till 2019. \\\n",
        "Geographical Scope: The WORKSITE_STATE gives information about the state where the primary worksite location. The data set covers all the states of US. \n",
        "The Data set also cover different Industries majority of the applications are for IT industry but not limited to IT, it also covers Medical, Pharmacy, Law, Carpentery etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KcVj-dd2vjUC",
        "colab": {}
      },
      "source": [
        "cleaned.SOC_TITLE.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XljvOXxQscAQ",
        "colab": {}
      },
      "source": [
        "cleaned.WORKSITE_STATE.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bwN87sFixRWt"
      },
      "source": [
        "####Faithfulness:\n",
        "Each record in the dataframe represents an application filled by the employer, hence we see a lot of spelling mistakes and some punctutaions like ,(comma) and some numbers in the JOB_TITLE, SOC_TITLE feilds, but there are no inconsistencies and data falsification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jlzTPhST7G0B",
        "colab": {}
      },
      "source": [
        "#The Job_title and Soc_title has inconsistencies such as numeric characters and symbols so it has to be cleaned\n",
        "cleaned['JOB_TITLE']=cleaned.JOB_TITLE.apply(lambda txt: \" \".join([cw.remove_num(i) for i in txt.lower().split()]))\n",
        "cleaned['JOB_TITLE']=cleaned['JOB_TITLE'].str.replace(',', '')\n",
        "\n",
        "cleaned['SOC_TITLE']=cleaned.SOC_TITLE.apply(lambda txt: \" \".join([cw.remove_num(i) for i in txt.lower().split()]))\n",
        "cleaned['SOC_TITLE']=cleaned['SOC_TITLE'].str.replace(',', '')\n",
        "cleaned['JOB_TITLE'].value_counts()\n",
        "warnings.simplefilter(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jHWSBSgF7pXX",
        "colab": {}
      },
      "source": [
        "#Function to clean data columns, to correct spelling mistakes and convert plural words to singular\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "words = set(nltk.corpus.words.words())\n",
        "spell = Speller()\n",
        "def lemmatize_text(text):\n",
        "     return lemmatizer.lemmatize(text)\n",
        "def spelling_checker(text):\n",
        "     return spell(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IpcwzJsJHb-d",
        "colab": {}
      },
      "source": [
        "cleaned['SOC_TITLE']=cleaned.SOC_TITLE.apply(lambda txt: \" \".join([lemmatize_text(i) for i in txt.lower().split()]))\n",
        "cleaned['SOC_TITLE']=cleaned.SOC_TITLE.apply(lambda txt: \" \".join([spelling_checker(i) for i in txt.lower().split()]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ssEwlv1n7xo1"
      },
      "source": [
        "The has lot of different subcategories which have less than 15 applications in them which does not show much significance , removing them help us condense the data and take only the significant rows.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GoOYWV-bCabm",
        "colab": {}
      },
      "source": [
        "cleaned= cw.drop_less_significant(cleaned)\n",
        "cleaned=cw.cat_to_num(cleaned)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IThvOHguobsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned.loc[(cleaned.CASE_STATUS == \"CERTIFIED\"),\"CASE_STATUS\"] = 1\n",
        "cleaned.loc[(cleaned.CASE_STATUS == \"DENIED\"),\"CASE_STATUS\"] = 0\n",
        "cleaned.loc[(cleaned.FULL_TIME_POSITION == 'Y'),\"FULL_TIME_POSITION\"] = 1\n",
        "cleaned.loc[(cleaned.FULL_TIME_POSITION == 'N'),\"FULL_TIME_POSITION\"] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPEVdBTo1ZC3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_scnt,data_anlst,data_eng,mach_learn=cw.data_jobs(cleaned)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eM8UzR-17Og",
        "colab_type": "text"
      },
      "source": [
        "## Are you a international student? Are you looking for jobs in Data Science domain?\n",
        "The below visualization shows median salary for four data science related jobs and the number of visa applications that are submitted for each job. This gives us insights about the jobs available for international students/workers about the job market for Data Science domain. We get to know the number of jobs available for each caegory as well as the median wage for each category. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH1xs65O2Dp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f,ax=plt.subplots(figsize=(8,6))\n",
        "bplot1=plt.boxplot([data_scnt[data_scnt['WAGES']<200000].WAGES,data_anlst[data_anlst['WAGES']<200000].WAGES,data_eng[data_eng['WAGES']<200000].WAGES,mach_learn[mach_learn['WAGES']<200000].WAGES],patch_artist=\"True\")\n",
        "ax.set_xticklabels(['Data Scientists','Data Analysts','Data Engineer','Machine Learning'],fontsize=15)\n",
        "ax.set_title('Salary Distribution for jobs in Data Science field in 2019', fontsize=15)\n",
        "ax.tick_params(labelsize=10)\n",
        "colors = ['blue','orange', 'green', 'red'] \n",
        "for patch, color in zip(bplot1['boxes'], colors): \n",
        "  patch.set_facecolor(color)\n",
        "plt.show()\n",
        "datajobs=cw.data_concat(pd,data_scnt,data_anlst,data_eng,mach_learn)\n",
        "ax2=sns.countplot(x=\"data\", data=datajobs)\n",
        "ax2.set_title('Number of petitions for each jobs in Data Science field in 2019', fontsize=15)\n",
        "warnings.simplefilter(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52vYSJW8FjF5",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### **Baseline classifier**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lLzCf96GZiD",
        "colab_type": "text"
      },
      "source": [
        "The baseline classifier is done with a basic model. In this case we are taking the mean of the labels ('certified' and 'denied' for H1B visa approvals). It will give us the base accuracy to which we will compare our classifier's accuracy. Our classifier should have a better accuracy than the baseline classifier accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCDCRjAqF1TB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This step assigns a binary class label (0 or 1) to each label for H1B visa approval. \n",
        "def create_class_labels(processed_data):\n",
        "    y = processed_data.to_numpy().astype(int)\n",
        "    return y\n",
        "X = cleaned['CASE_STATUS'].to_numpy()\n",
        "y = create_class_labels(cleaned['CASE_STATUS']) # Groundtruth labels for the dataset\n",
        "counts = cleaned['CASE_STATUS'].value_counts()\n",
        "print(counts)\n",
        "print('proportion: ', counts[1]/counts[0], ': 1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH_ZEZHYF4Ld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_accuracy(validation, predicted):\n",
        "    comp = prediction == validation \n",
        "    match_counts = np.count_nonzero(comp == True) \n",
        "    clasifier_accuracy = match_counts/len(validation)\n",
        "    return clasifier_accuracy  \n",
        "def compute_AUC(y, prediction):\n",
        "    auc = None\n",
        "    try:\n",
        "        auc = roc_auc_score(y, prediction)\n",
        "    except ValueError:\n",
        "        pass\n",
        "    return auc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBm_dFDZF8LD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracies = []\n",
        "AUCs = []\n",
        "kf = sklearn.model_selection.KFold(n_splits=4, random_state=1, shuffle=True)# Testing with K-folds \n",
        "for train_idx, test_idx in kf.split(X):\n",
        "    X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx] \n",
        "    prediction = blc.run_clasifier(X_train, y_train, X_test, np)\n",
        "    fold_accuracy = compute_accuracy(y_test, prediction)\n",
        "    fold_AUC = compute_AUC(y_test, prediction)\n",
        "    accuracies.append(fold_accuracy)\n",
        "    if fold_AUC != None: AUCs.append(fold_AUC)\n",
        "baseline_clasifier_accuracy = mean(accuracies)\n",
        "print('Baseline accuracy (K-fold): ', baseline_clasifier_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpX2h3fhF_Du",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)# Testing with regular split\n",
        "prediction = blc.run_clasifier(X_train, y_train, X_test, np)\n",
        "split_accuracy = compute_accuracy(y_test, prediction)\n",
        "split_AUC = compute_AUC(y_test, prediction)\n",
        "print('Baseline accuracy (split): ', split_accuracy)  \n",
        "print('AUC K-fold: ', mean(AUCs))\n",
        "print('AUC split (80-20): ', split_AUC)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1BXDP3TGD8-",
        "colab_type": "text"
      },
      "source": [
        "The accuracy results of the baseline classifier is 0.99. This result is due to the highly imbalanced data, where there are 624682 CERTIFIED applications and 5158 DENIED applications. The proportion of it is 121.10934470725087 to 1. Therefore, a performance measure based on the accuracy is not a good one. A better performance measure in imbalanced data is the Area under the ROC Curve (AUC). It meassures the likelihood that given two random points (one from the positive and one from the negative class) the classifier will rank the point from the positive class higher than the one from the negative one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H3wXPnEnse3",
        "colab_type": "text"
      },
      "source": [
        "**Features considered for the classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNftZfY8lCCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Dataset = cleaned[[\"CASE_STATUS\", \"FULL_TIME_POSITION\",\"SOC_TITLE\", \"WORKSITE_STATE\", \"WAGES\", \"H1B_DEPENDENT\"]]\n",
        "Dataset.head()\n",
        "Dataset.reset_index(drop = True,inplace = True)\n",
        "Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJqE8futomje",
        "colab_type": "text"
      },
      "source": [
        "CONVERTING CATEGORICAL COLUMNS TO NUMERIC COLUMNS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5QiHud8lUTI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_punctuation(value):\n",
        "    result = \"\"\n",
        "    for c in value:\n",
        "        if c not in string.punctuation:\n",
        "            result += c\n",
        "        else:\n",
        "            result +=\" \"\n",
        "    return result\n",
        "Dataset[\"SOC_TITLE\"] = Dataset[\"SOC_TITLE\"].apply(lambda x : remove_punctuation(x))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg6RHFPqpD4k",
        "colab_type": "text"
      },
      "source": [
        "If we have more number of unique values in the categorical columns we can use word_vectors to convert them into numeric columns as one-hot encoding fails to convert."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEQmkJJ1lndg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_vectors_1 = api.load(\"glove-wiki-gigaword-100\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzg1qFcIl2Jh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(columns = [\"job_position\"])\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnHViS-BmAgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# converting job titles to numeric values\n",
        "def grouping(position):\n",
        "  # print(position)\n",
        "  z = np.zeros(100)\n",
        "  x = position.split()\n",
        "  count = 0\n",
        "  for i in x:\n",
        "    if i in word_vectors_1:\n",
        "      z += word_vectors_1[i]\n",
        "      count+=1\n",
        "  if count>0:\n",
        "    return z/count\n",
        "  else:\n",
        "    return z\n",
        "df[\"job_position\"] = Dataset[\"SOC_TITLE\"].apply(lambda text : grouping(text))     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfGdfLUTmDBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " Dataset.drop(columns=[\"SOC_TITLE\"],axis=1,inplace=True)\n",
        "Dataset.columns\n",
        "Dataset.reset_index(drop = True, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdPXmezNmKZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one hot encoding to convert states column to numeric values using one hot encoding\n",
        "Encoding = OneHotEncoder(handle_unknown='ignore',sparse = True)\n",
        "Encoding_df = pd.DataFrame(Encoding.fit_transform(Dataset[[\"WORKSITE_STATE\"]]).toarray())\n",
        "Dataset = Dataset.join(Encoding_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkZK5mdxma5U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Dataset.drop(columns=['WORKSITE_STATE'],axis=1,inplace=True)\n",
        "word_vectors = df[[\"job_position\"]].values\n",
        "word_vectors.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4THhCcLm1-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp = np.vstack(word_vectors.tolist())\n",
        "temp.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk1xerqrquLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Y represents the output labels\n",
        "Y = Dataset[\"CASE_STATUS\"].astype(float)\n",
        "Dataset.drop(columns = [\"CASE_STATUS\"],inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGHyM_iTnEiD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# all_features represent the input labels\n",
        "other_features = Dataset.iloc[:].values\n",
        "all_features = np.hstack([temp, other_features])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSqJobhYOj-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Dependent and independent variables\n",
        "X = all_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrMgSvnoPMN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Split the date into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IXHXqRSPUfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Build model - Random Forest Classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#model_clf = RandomForestClassifier(n_estimators=10, random_state=30)\n",
        "model_clf = RandomForestClassifier(n_jobs=2,random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzWotUaEPX9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train the model\n",
        "model_clf.fit(X_train,y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GOkp9nvPbkU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test the model (predict with our test data)\n",
        "prediction_test = model_clf.predict(X_test)\n",
        "prediction_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9xAQ6fCPict",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#compare with original value, Y_test\n",
        "from sklearn import metrics\n",
        "print(\"Accuracy = \", metrics.accuracy_score(y_test, prediction_test))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}