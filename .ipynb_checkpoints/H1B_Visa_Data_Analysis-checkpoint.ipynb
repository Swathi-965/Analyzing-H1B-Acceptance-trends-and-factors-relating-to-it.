{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-S6Q8i5yQ_zF"
   },
   "source": [
    "## Analysing H1B Visa data Trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G3DkkMAuRQ9j"
   },
   "source": [
    "H1B visa is a nonimmigrant visa issued to gradaute level applicants allowing them to work in the United States. The employer sponsors the H1B visa for workers with theoretical or technical expertise in specialized fields such as in IT, finance, accounting etc. An interesting fact about immigrant workers is that about 52 percent of new Silicon valley companies were founded by such workers during 1995 and 2005. Some famous CEOs like Indira Nooyi (Pepsico), Elon Musk (Tesla), Sundar Pichai (Google),Satya Nadella (Microsoft) once arrived to the US on a H1B visa.\\\n",
    "**Motivation**: Our team consists of five international graduate students, in the future we will be applying for H1B visa. The visa application process seems very long, complicated and uncertain. So we decided to understand this process and use Machine learning algorithms to predict the acceptance rate and trends of H1B visa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w978Te7MRoFx"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lmhgvIddSRf0"
   },
   "outputs": [],
   "source": [
    "!pip install autocorrect\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk,warnings\n",
    "import clean_wage as cw\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from statistics import mean\n",
    "from autocorrect import Speller \n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import OneHotEncoder #ONE HOT ENCODING\n",
    "from sklearn.ensemble import RandomForestClassifier #Build model - Random Forest Classifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ap8NQkonSOJq"
   },
   "source": [
    "### Data \n",
    "The data used in the project has been collected from <a href=\"https://www.foreignlaborcert.doleta.gov/performancedata.cfm\">the Office of Foreign Labor Certification (OFLC).</a>The Data provides insight into each petition with information such as the Job title, Wage, Employer, Worksite location etc. To get the dataset click on the above link-> click on Disclosure data -> scroll down to H1B data.\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ztV8S9xS40A"
   },
   "outputs": [],
   "source": [
    "file2015=pd.read_csv('/content/gdrive/My Drive/H1B_project/H-1B_Disclosure_Data_FY15_Q4.csv',encoding='latin-1', low_memory=False)\n",
    "file2015.rename(columns = {'H-1B_DEPENDENT':'H1B_DEPENDENT','SOC_NAME':'SOC_TITLE','WAGE_RATE_OF_PAY':'WAGE_RATE_OF_PAY_FROM'}, inplace = True)\n",
    "df2015=file2015[['CASE_NUMBER', 'CASE_STATUS', 'CASE_SUBMITTED', 'DECISION_DATE',\n",
    "       'VISA_CLASS','FULL_TIME_POSITION','JOB_TITLE', 'SOC_CODE', 'SOC_TITLE','EMPLOYER_NAME','WAGE_RATE_OF_PAY_FROM',\n",
    "       'WAGE_UNIT_OF_PAY','WORKSITE_CITY','WORKSITE_STATE','H1B_DEPENDENT']]\n",
    "df2015['WAGE_RATE_OF_PAY_FROM']=df2015['WAGE_RATE_OF_PAY_FROM'].str.split('-').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nc0KI4GHTDTF"
   },
   "outputs": [],
   "source": [
    "file2016=pd.read_csv('/content/gdrive/My Drive/H1B_project/H-1B_Disclosure_Data_FY16.csv',encoding='latin-1', low_memory=False)\n",
    "file2016.rename(columns = {'H-1B_DEPENDENT':'H1B_DEPENDENT','SOC_NAME':'SOC_TITLE'}, inplace = True)\n",
    "df2016=file2016[['CASE_NUMBER', 'CASE_STATUS', 'CASE_SUBMITTED', 'DECISION_DATE',\n",
    "       'VISA_CLASS','FULL_TIME_POSITION','JOB_TITLE', 'SOC_CODE', 'SOC_TITLE','EMPLOYER_NAME','WAGE_RATE_OF_PAY_FROM',\n",
    "       'WAGE_UNIT_OF_PAY','WORKSITE_CITY','WORKSITE_STATE','H1B_DEPENDENT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B1r1pOSrTHgc"
   },
   "outputs": [],
   "source": [
    "file2017=pd.read_csv('/content/gdrive/My Drive/H1B_project/H-1B_Disclosure_Data_FY17.csv',encoding='latin-1', low_memory=False)\n",
    "file2017.rename(columns = {'SOC_NAME':'SOC_TITLE'}, inplace = True)\n",
    "df2017=file2017[['CASE_NUMBER', 'CASE_STATUS', 'CASE_SUBMITTED', 'DECISION_DATE',\n",
    "       'VISA_CLASS','FULL_TIME_POSITION','JOB_TITLE', 'SOC_CODE', 'SOC_TITLE','EMPLOYER_NAME','WAGE_RATE_OF_PAY_FROM',\n",
    "       'WAGE_UNIT_OF_PAY','WORKSITE_CITY','WORKSITE_STATE','H1B_DEPENDENT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kj0d8XrfTPd1"
   },
   "outputs": [],
   "source": [
    "file2018=pd.read_csv('/content/gdrive/My Drive/H1B_project/H-1B_Disclosure_Data_FY2018_EOY.csv',encoding='latin-1', low_memory=False)\n",
    "file2018.rename(columns = {'SOC_NAME':'SOC_TITLE'}, inplace = True)\n",
    "df2018=file2018[['CASE_NUMBER', 'CASE_STATUS', 'CASE_SUBMITTED', 'DECISION_DATE',\n",
    "       'VISA_CLASS','FULL_TIME_POSITION','JOB_TITLE', 'SOC_CODE', 'SOC_TITLE','EMPLOYER_NAME','WAGE_RATE_OF_PAY_FROM',\n",
    "       'WAGE_UNIT_OF_PAY','WORKSITE_CITY','WORKSITE_STATE','H1B_DEPENDENT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C6GtHpVRTWet"
   },
   "outputs": [],
   "source": [
    "file2019=pd.read_csv('/content/gdrive/My Drive/H1B_project/H-1B_Disclosure_Data_FY2019.csv',encoding='latin-1', low_memory=False)\n",
    "file2019.rename(columns = {'H-1B_DEPENDENT':'H1B_DEPENDENT','WAGE_RATE_OF_PAY_FROM_1':'WAGE_RATE_OF_PAY_FROM','WAGE_UNIT_OF_PAY_1':'WAGE_UNIT_OF_PAY',\\\n",
    "                           'WORKSITE_CITY_1':'WORKSITE_CITY','WORKSITE_STATE_1':'WORKSITE_STATE'}, inplace = True)\n",
    "df2019=file2019[['CASE_NUMBER', 'CASE_STATUS', 'CASE_SUBMITTED', 'DECISION_DATE',\n",
    "       'VISA_CLASS','FULL_TIME_POSITION','JOB_TITLE', 'SOC_CODE', 'SOC_TITLE','EMPLOYER_NAME','WAGE_RATE_OF_PAY_FROM',\n",
    "       'WAGE_UNIT_OF_PAY','WORKSITE_CITY','WORKSITE_STATE','H1B_DEPENDENT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qCok1RY0VFeM"
   },
   "outputs": [],
   "source": [
    "cleaned=pd.concat([df2015,df2016,df2017,df2018,df2019])\n",
    "cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QngD94-VEmRu"
   },
   "outputs": [],
   "source": [
    "del file2015 # To avoid using the RAM completely\n",
    "del file2016\n",
    "del file2017\n",
    "del file2018\n",
    "del file2019\n",
    "del df2019\n",
    "del df2018\n",
    "del df2017\n",
    "del df2016\n",
    "del df2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S2RKhcbJycxS"
   },
   "source": [
    "### Exploratory Data Analysis\n",
    "Before we begin working on our data we need to understand the traits of our data which we accomplish using EDA. We see that we have about 260 columns , not all 260 columns have essential information that contributes to our analysis. Hence we pick out the columns such as case status( Accepted/ Denied) ,Employer, Job title etc.We can look at all the colums and the types of object in each column using cleaned.info(). We also use cleaned['column_name'].value_counts() to find the classes in the column along with the count. For feature engineering we are converting quantitative data to categorical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jEaANag2w5Uk"
   },
   "source": [
    "####Granularity:\n",
    "Each row in the dataframe represents an application that was filled by the employer and some values are added during the processing of the application such as DECISION_DATE, CASE_STATUS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YI8muywco9DA"
   },
   "outputs": [],
   "source": [
    "cleaned.dropna(inplace=True)\n",
    "cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oG0hPOcJI78r"
   },
   "outputs": [],
   "source": [
    "cleaned.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y5Pbv_3_mkH_"
   },
   "outputs": [],
   "source": [
    "cleaned['VISA_CLASS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k37klRspm4BW"
   },
   "outputs": [],
   "source": [
    "cleaned.drop(labels=cleaned.loc[cleaned['VISA_CLASS']!='H-1B'].index , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "efuJruAvnNMV"
   },
   "outputs": [],
   "source": [
    "cleaned['CASE_STATUS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4GXmkkTrnBaI"
   },
   "outputs": [],
   "source": [
    "cleaned.drop(labels=cleaned.loc[cleaned['CASE_STATUS']=='WITHDRAWN'].index , inplace=True)\n",
    "cleaned.drop(labels=cleaned.loc[cleaned['CASE_STATUS']=='CERTIFIED-WITHDRAWN'].index , inplace=True)\n",
    "cleaned['CASE_STATUS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3aOpaG2Dohr7"
   },
   "outputs": [],
   "source": [
    "cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ebgMvKVMor6Z"
   },
   "outputs": [],
   "source": [
    "cleaned['WAGE_RATE_OF_PAY_FROM'].apply(type).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJ-yRmY0ehSJ"
   },
   "source": [
    "####Temporality\n",
    "The data is available from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M_EkmaiQegGn"
   },
   "outputs": [],
   "source": [
    "max_date=cleaned.CASE_SUBMITTED.max()\t\n",
    "min_date=cleaned.CASE_SUBMITTED.min()\n",
    "print(\"The date in the dataset ranges from \"+str(max_date)+\" to\",min_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nbfyEEmngLY2"
   },
   "outputs": [],
   "source": [
    "cleaned['WAGES']=cleaned['WAGE_RATE_OF_PAY_FROM'].apply(cw.clean_wages).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VDi3g4fIgRDm"
   },
   "outputs": [],
   "source": [
    "cleaned['WAGE_UNIT_OF_PAY'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VBCLazyqgXxu"
   },
   "outputs": [],
   "source": [
    "cleaned= cw.clean_wageUnit(np,cleaned)\n",
    "cleaned.drop(columns=['WAGE_RATE_OF_PAY_FROM','WAGE_UNIT_OF_PAY'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vhMx0hpTjlRR"
   },
   "outputs": [],
   "source": [
    "print(cleaned['WAGES'].describe())\n",
    "sns.boxplot(y=cleaned['WAGES'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UEGwyt3ukJvG"
   },
   "source": [
    "We can see that we have large outliers , we can remove them from the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2uWva-1ulVgx"
   },
   "outputs": [],
   "source": [
    "# Remove the outliers.\n",
    "lowerBound = 0.001\n",
    "upperBound = 0.95\n",
    "result = cleaned.WAGES.quantile([lowerBound, upperBound])\n",
    "cleaned = cleaned[(result.loc[lowerBound] < cleaned.WAGES.values) &\\\n",
    "                  (cleaned.WAGES.values < result.loc[upperBound])]\n",
    "\n",
    "print(cleaned['WAGES'].describe())\n",
    "sns.boxplot(y=cleaned['WAGES'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vJ-OdEdRp_TU"
   },
   "outputs": [],
   "source": [
    "Top_Employer=cleaned['EMPLOYER_NAME'].value_counts()[:10]\n",
    "plt.figure(figsize=[8,8])\n",
    "ax=sns.barplot(y=Top_Employer.index,x=Top_Employer.values,palette=sns.color_palette('viridis',10))\n",
    "ax.tick_params(labelsize=12)\n",
    "for i, v in enumerate(Top_Employer.values): \n",
    "    ax.text(.5, i, v,fontsize=15,color='white',weight='bold')\n",
    "plt.title('Top 10 Companies sponsoring H1B Visa in 2019', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XW2MrCJ9rkx5"
   },
   "outputs": [],
   "source": [
    "cleaned.WORKSITE_STATE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "30UMuuYNrzJT"
   },
   "outputs": [],
   "source": [
    "cleaned= cw.clean_states(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6y8zxWPftW6U"
   },
   "source": [
    "#### Data Scope\n",
    "The data available contains applications submitted by Employers from 2015 till 2019. \\\n",
    "Geographical Scope: The WORKSITE_STATE gives information about the state where the primary worksite location. The data set covers all the states of US. \n",
    "The Data set also cover different Industries majority of the applications are for IT industry but not limited to IT, it also covers Medical, Pharmacy, Law, Carpentery etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KcVj-dd2vjUC"
   },
   "outputs": [],
   "source": [
    "cleaned.SOC_TITLE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XljvOXxQscAQ"
   },
   "outputs": [],
   "source": [
    "cleaned.WORKSITE_STATE.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bwN87sFixRWt"
   },
   "source": [
    "####Faithfulness:\n",
    "Each record in the dataframe represents an application filled by the employer, hence we see a lot of spelling mistakes and some punctutaions like ,(comma) and some numbers in the JOB_TITLE, SOC_TITLE feilds, but there are no inconsistencies and data falsification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jlzTPhST7G0B"
   },
   "outputs": [],
   "source": [
    "#The Job_tite and Soc_title had lot numbers in between and ,(comma) so it has to be cleaned\n",
    "cleaned['JOB_TITLE']=cleaned.JOB_TITLE.apply(lambda txt: \" \".join([cw.remove_num(i) for i in txt.lower().split()]))\n",
    "cleaned['JOB_TITLE']=cleaned['JOB_TITLE'].str.replace(',', '')\n",
    "\n",
    "cleaned['SOC_TITLE']=cleaned.SOC_TITLE.apply(lambda txt: \" \".join([cw.remove_num(i) for i in txt.lower().split()]))\n",
    "cleaned['SOC_TITLE']=cleaned['SOC_TITLE'].str.replace(',', '')\n",
    "cleaned['JOB_TITLE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jHWSBSgF7pXX"
   },
   "outputs": [],
   "source": [
    "#Function to clean data columns, to correct spelling mistakes and convert plural words to singular\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "words = set(nltk.corpus.words.words())\n",
    "spell = Speller()\n",
    "def lemmatize_text(text):\n",
    "     return lemmatizer.lemmatize(text)\n",
    "def spelling_checker(text):\n",
    "     return spell(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m0dB4Db87quJ"
   },
   "outputs": [],
   "source": [
    "cleaned['JOB_TITLE']=cleaned.JOB_TITLE.apply(lambda txt: \" \".join([lemmatize_text(i) for i in txt.lower().split()]))\n",
    "cleaned['JOB_TITLE']=cleaned.JOB_TITLE.apply(lambda txt: \" \".join([spelling_checker(i) for i in txt.lower().split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IpcwzJsJHb-d"
   },
   "outputs": [],
   "source": [
    "cleaned['SOC_TITLE']=cleaned.SOC_TITLE.apply(lambda txt: \" \".join([lemmatize_text(i) for i in txt.lower().split()]))\n",
    "cleaned['SOC_TITLE']=cleaned.SOC_TITLE.apply(lambda txt: \" \".join([spelling_checker(i) for i in txt.lower().split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ssEwlv1n7xo1"
   },
   "source": [
    "The has lot of different subcategories which have less than 15 applications in them which does not show much significance , removing them help us condense the data and take only the significant rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GoOYWV-bCabm"
   },
   "outputs": [],
   "source": [
    "cleaned= cw.drop_less_significant(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned.loc[(cleaned.CASE_STATUS == \"CERTIFIED\"),\"CASE_STATUS\"] = 1\n",
    "cleaned.loc[(cleaned.CASE_STATUS == \"DENIED\"),\"CASE_STATUS\"] = 0\n",
    "cleaned.loc[(cleaned.FULL_TIME_POSITION == 'Y'),\"FULL_TIME_POSITION\"] = 1\n",
    "cleaned.loc[(cleaned.FULL_TIME_POSITION == 'N'),\"FULL_TIME_POSITION\"] = 0"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "H1B_Visa_Data_Analysis.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
